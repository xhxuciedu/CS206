{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xhxuciedu/CS206/blob/master/Transformers_and_MHAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "493b0f6qnTS2"
      },
      "source": [
        "# Tutorial 6: Transformers and Multi-Head Attention\n",
        "\n",
        "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
        "\n",
        "\n",
        "**Filled notebook:**\n",
        "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.ipynb)\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.ipynb)  \n",
        "**Pre-trained models:**\n",
        "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial6)\n",
        "[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/1DF7POc6j03pRiWQPWSl5QJX5iY-xK0sV?usp=sharing)   \n",
        "**Recordings:**\n",
        "[![YouTube - Part 1](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%201&color=red)](https://youtu.be/hGZ6wa07Vak)\n",
        "[![YouTube - Part 2](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%202&color=red)](https://youtu.be/QdTgJ85E6YA)\n",
        "[![YouTube - Part 3](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%203&color=red)](https://youtu.be/e7xvF2yS4Dg)    \n",
        "**JAX+Flax version:**\n",
        "[![View on RTD](https://img.shields.io/static/v1.svg?logo=readthedocs&label=RTD&message=View%20On%20RTD&color=8CA1AF)](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial6/Transformers_and_MHAttention.html)   \n",
        "**Author:** Phillip Lippe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk4aCZCbnTS7"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "\n",
        "**Note:** Interested in JAX? Check out our [JAX+Flax version](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial6/Transformers_and_MHAttention.html) of this tutorial!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTxytMAnnTS7"
      },
      "source": [
        "In this tutorial, we will discuss one of the most impactful architectures of the last 2 years: the Transformer model. Since the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. had been published in 2017, the Transformer architecture has continued to beat benchmarks in many domains, most importantly in Natural Language Processing. Transformers with an incredible amount of parameters can generate long, convincing [essays](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3), and opened up new application fields of AI. As the hype of the Transformer architecture seems not to come to an end in the next years, it is important to understand how it works, and have implemented it yourself, which we will do in this notebook.\n",
        "\n",
        "Despite the huge success of Transformers in NLP, we will _not_ include the NLP domain in our notebook here. Why? Firstly, the Master AI at UvA offers many great NLP courses that will take a closer look at the application of the Transformer architecture in NLP ([NLP2](https://studiegids.uva.nl/xmlpages/page/2020-2021/zoek-vak/vak/79628), [Advanced Topics in Computational Semantics](https://studiegids.uva.nl/xmlpages/page/2020-2021/zoek-vak/vak/80162)). Secondly, assignment 2 takes already a closer look at language generation on character level, on which you could easily apply our transformer architecture. Finally, and most importantly, there is so much more to the Transformer architecture. NLP is the domain the Transformer architecture has been originally proposed for and had the greatest impact on, but it also accelerated research in other domains, recently even [Computer Vision](https://arxiv.org/abs/2010.11929). Thus, we focus here on what makes the Transformer and self-attention so powerful in general. In [Tutorial 15](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html), we will discuss the application of Transformers in Computer Vision.\n",
        "\n",
        "Below, we import our standard libraries. Similarly as in Tutorial 5, we will use [PyTorch Lightning](https://www.pytorchlightning.ai/) as an additional framework. If you are not familiar with PyTorch Lightning, please make sure to have read Tutorial 5 carefully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "0I4IyLcFnTS8",
        "outputId": "af32e74e-8485-45f8-eee3-e1058320375e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-46060d299820>:14: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf') # For export\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import json\n",
        "from functools import partial\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision import transforms\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial6\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mgejwWinTS-"
      },
      "source": [
        "Two pre-trained models are downloaded below. Make sure to have adjusted your `CHECKPOINT_PATH` before running this code if not already done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4kKcKzhnTS_",
        "outputId": "269710d9-5eb9-4ac3-e077-6704ddb5bf17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/ReverseTask.ckpt...\n",
            "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/SetAnomalyTask.ckpt...\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/\"\n",
        "# Files to download\n",
        "pretrained_files = [\"ReverseTask.ckpt\", \"SetAnomalyTask.ckpt\"]\n",
        "\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if \"/\" in file_name:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUUVbS9_nTTA"
      },
      "source": [
        "## The Transformer architecture\n",
        "\n",
        "In the first part of this notebook, we will implement the Transformer architecture by hand. As the architecture is so popular, there already exists a Pytorch module `nn.Transformer` ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)) and a [tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) on how to use it for next token prediction. However, we will implement it here ourselves, to get through to the smallest details.\n",
        "\n",
        "There are of course many more tutorials out there about attention and Transformers. Below, we list a few that are worth exploring if you are interested in the topic and might want yet another perspective on the topic after this one:\n",
        "\n",
        "* [Transformer: A Novel Neural Network Architecture for Language Understanding (Jakob Uszkoreit, 2017)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) - The original Google blog post about the Transformer paper, focusing on the application in machine translation.\n",
        "* [The Illustrated Transformer (Jay Alammar, 2018)](http://jalammar.github.io/illustrated-transformer/) - A very popular and great blog post intuitively explaining the Transformer architecture with many nice visualizations. The focus is on NLP.\n",
        "* [Attention? Attention! (Lilian Weng, 2018)](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) - A nice blog post summarizing attention mechanisms in many domains including vision.\n",
        "* [Illustrated: Self-Attention (Raimi Karim, 2019)](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) - A nice visualization of the steps of self-attention. Recommended going through if the explanation below is too abstract for you.\n",
        "* [The Transformer family (Lilian Weng, 2020)](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html) - A very detailed blog post reviewing more variants of Transformers besides the original one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3pX40w3nTTA"
      },
      "source": [
        "### What is Attention?\n",
        "\n",
        "The attention mechanism describes a recent new group of layers in neural networks that has attracted a lot of interest in the past few years, especially in sequence tasks. There are a lot of different possible definitions of \"attention\" in the literature, but the one we will use here is the following: _the attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements' keys_. So what does this exactly mean? The goal is to take an average over the features of multiple elements. However, instead of weighting each element equally, we want to weight them depending on their actual values. In other words, we want to dynamically decide on which inputs we want to \"attend\" more than others. In particular, an attention mechanism has usually four parts we need to specify:\n",
        "\n",
        "* **Query**: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
        "* **Keys**: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is \"offering\", or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
        "* **Values**: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
        "* **Score function**: To rate which elements we want to pay attention to, we need to specify a score function $f_{attn}$. The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
        "\n",
        "\n",
        "The weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query. If we try to describe it with pseudo-math, we can write:\n",
        "\n",
        "$$\n",
        "\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{key}_i, \\text{query}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{key}_j, \\text{query}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{value}_i\n",
        "$$\n",
        "\n",
        "Visually, we can show the attention over a sequence of words as follows:\n",
        "\n",
        "<center width=\"100%\" style=\"padding:25px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/attention_example.svg?raw=1\" width=\"750px\"></center>\n",
        "\n",
        "For every word, we have one key and one value vector. The query is compared to all keys with a score function (in this case the dot product) to determine the weights. The softmax is not visualized for simplicity. Finally, the value vectors of all words are averaged using the attention weights.\n",
        "\n",
        "Most attention mechanisms differ in terms of what queries they use, how the key and value vectors are defined, and what score function is used. The attention applied inside the Transformer architecture is called **self-attention**. In self-attention, each sequence element provides a key, value, and query. For each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements' keys, and returned a different, averaged value vector for each element. We will now go into a bit more detail by first looking at the specific implementation of the attention mechanism which is in the Transformer case the scaled dot product attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SspyvcIqnTTB"
      },
      "source": [
        "### Scaled Dot Product Attention\n",
        "\n",
        "The core concept behind self-attention is the scaled dot product attention. Our goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute. The dot product attention takes as input a set of queries $Q\\in\\mathbb{R}^{T\\times d_k}$, keys $K\\in\\mathbb{R}^{T\\times d_k}$ and values $V\\in\\mathbb{R}^{T\\times d_v}$ where $T$ is the sequence length, and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively. For simplicity, we neglect the batch dimension for now. The attention value from element $i$ to $j$ is based on its similarity of the query $Q_i$ and key $K_j$, using the dot product as the similarity metric. In math, we calculate the dot product attention as follows:\n",
        "\n",
        "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "The matrix multiplication $QK^T$ performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape $T\\times T$. Each row represents the attention logits for a specific element $i$ to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention). Another perspective on this attention mechanism offers the computation graph which is visualized below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/scaled_dot_product_attn.svg?raw=1\" width=\"210px\"></center>\n",
        "\n",
        "One aspect we haven't discussed yet is the scaling factor of $1/\\sqrt{d_k}$. This scaling factor is crucial to maintain an appropriate variance of attention values after initialization. Remember that we intialize our layers with the intention of having equal variance throughout the model, and hence, $Q$ and $K$ might also have a variance close to $1$. However, performing a dot product over two vectors with a variance $\\sigma^2$ results in a scalar having $d_k$-times higher variance:\n",
        "\n",
        "$$q_i \\sim \\mathcal{N}(0,\\sigma^2), k_i \\sim \\mathcal{N}(0,\\sigma^2) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma^4\\cdot d_k$$\n",
        "\n",
        "\n",
        "If we do not scale down the variance back to $\\sim\\sigma^2$, the softmax over the logits will already saturate to $1$ for one random element and $0$ for all others. The gradients through the softmax will be close to zero so that we can't learn the parameters appropriately. Note that the extra factor of $\\sigma^2$, i.e., having $\\sigma^4$ instead of $\\sigma^2$, is usually not an issue, since we keep the original variance $\\sigma^2$ close to $1$ anyways.\n",
        "\n",
        "The block `Mask (opt.)` in the diagram above represents the optional masking of specific entries in the attention matrix. This is for instance used if we stack multiple sequences with different lengths into a batch. To still benefit from parallelization in PyTorch, we pad the sentences to the same length and mask out the padding tokens during the calculation of the attention values. This is usually done by setting the respective attention logits to a very low value.\n",
        "\n",
        "After we have discussed the details of the scaled dot product attention block, we can write a function below which computes the output features given the triple of queries, keys, and values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8FXT9YuqnTTB"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DzEqiLSnTTB"
      },
      "source": [
        "Note that our code above supports any additional dimensionality in front of the sequence length so that we can also use it for batches. However, for a better understanding, let's generate a few random queries, keys, and value vectors, and calculate the attention outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVzexxgLnTTC",
        "outputId": "915dc336-d133-452f-f06f-81f6bbacc515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " tensor([[ 0.3367,  0.1288],\n",
            "        [ 0.2345,  0.2303],\n",
            "        [-1.1229, -0.1863]])\n",
            "K\n",
            " tensor([[ 2.2082, -0.6380],\n",
            "        [ 0.4617,  0.2674],\n",
            "        [ 0.5349,  0.8094]])\n",
            "V\n",
            " tensor([[ 1.1103, -1.6898],\n",
            "        [-0.9890,  0.9580],\n",
            "        [ 1.3221,  0.8172]])\n",
            "Values\n",
            " tensor([[ 0.5698, -0.1520],\n",
            "        [ 0.5379, -0.0265],\n",
            "        [ 0.2246,  0.5556]])\n",
            "Attention\n",
            " tensor([[0.4028, 0.2886, 0.3086],\n",
            "        [0.3538, 0.3069, 0.3393],\n",
            "        [0.1303, 0.4630, 0.4067]])\n"
          ]
        }
      ],
      "source": [
        "seq_len, d_k = 3, 2\n",
        "pl.seed_everything(42)\n",
        "q = torch.randn(seq_len, d_k)\n",
        "k = torch.randn(seq_len, d_k)\n",
        "v = torch.randn(seq_len, d_k)\n",
        "values, attention = scaled_dot_product(q, k, v)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"Values\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_sY7HiXnTTC"
      },
      "source": [
        "Before continuing, make sure you can follow the calculation of the specific values here, and also check it by hand. It is important to fully understand how the scaled dot product attention is calculated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEmmm_tDnTTC"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. Specifically, given a query, key, and value matrix, we transform those into $h$ sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix. Mathematically, we can express this operation as:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
        "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "We refer to this as Multi-Head Attention layer with the learnable parameters $W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}$, and $W^{O}\\in\\mathbb{R}^{h\\cdot d_v\\times d_{out}}$ ($D$ being the input dimensionality). Expressed in a computational graph, we can visualize it as below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/multihead_attention.svg?raw=1\" width=\"230px\"></center>\n",
        "\n",
        "How are we applying a Multi-Head Attention layer in a neural network, where we don't have an arbitrary query, key, and value vector as input? Looking at the computation graph above, a simple but effective implementation is to set the current feature map in a NN, $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$, as $Q$, $K$ and $V$ ($B$ being the batch size, $T$ the sequence length, $d_{\\text{model}}$ the hidden dimensionality of $X$). The consecutive weight matrices $W^{Q}$, $W^{K}$, and $W^{V}$ can transform $X$ to the corresponding feature vectors that represent the queries, keys, and values of the input. Using this approach, we can implement the Multi-Head Attention module below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WJD9VYSUnTTC"
      },
      "outputs": [],
      "source": [
        "# Helper function to support different mask shapes.\n",
        "# Output shape supports (batch_size, number of heads, seq length, seq length)\n",
        "# If 2D: broadcasted over batch size and number of heads\n",
        "# If 3D: broadcasted over number of heads\n",
        "# If 4D: leave as is\n",
        "def expand_mask(mask):\n",
        "    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n",
        "    if mask.ndim == 3:\n",
        "        mask = mask.unsqueeze(1)\n",
        "    while mask.ndim < 4:\n",
        "        mask = mask.unsqueeze(0)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "st7Nwi4rnTTD"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        # Note that in many implementations you see \"bias=False\" which is optional\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "        if mask is not None:\n",
        "            mask = expand_mask(mask)\n",
        "        qkv = self.qkv_proj(x)\n",
        "\n",
        "        # Separate Q, K, V from linear output\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # Determine value outputs\n",
        "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
        "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
        "        o = self.o_proj(values)\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tLkZdo5nTTD"
      },
      "source": [
        "One crucial characteristic of the multi-head attention is that it is permutation-equivariant with respect to its inputs. This means that if we switch two input elements in the sequence, e.g. $X_1\\leftrightarrow X_2$ (neglecting the batch dimension for now), the output is exactly the same besides the elements 1 and 2 switched. Hence, the multi-head attention is actually looking at the input not as a sequence, but as a set of elements. This property makes the multi-head attention block and the Transformer architecture so powerful and widely applicable! But what if the order of the input is actually important for solving the task, like language modeling? The answer is to encode the position in the input features, which we will take a closer look at later (topic _Positional encodings_ below).\n",
        "\n",
        "Before moving on to creating the Transformer architecture, we can compare the self-attention operation with our other common layer competitors for sequence data: convolutions and recurrent neural networks. Below you can find a table by [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) on the complexity per layer, the number of sequential operations, and maximum path length. The complexity is measured by the upper bound of the number of operations to perform, while the maximum path length represents the maximum number of steps a forward or backward signal has to traverse to reach any other position. The lower this length, the better gradient signals can backpropagate for long-range dependencies. Let's take a look at the table below:\n",
        "\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/comparison_conv_rnn.svg?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "$n$ is the sequence length, $d$ is the representation dimension and $k$ is the kernel size of convolutions. In contrast to recurrent networks, the self-attention layer can parallelize all its operations making it much faster to execute for smaller sequence lengths. However, when the sequence length exceeds the hidden dimensionality, self-attention becomes more expensive than RNNs. One way of reducing the computational cost for long sequences is by restricting the self-attention to a neighborhood of inputs to attend over, denoted by $r$. Nevertheless, there has been recently a lot of work on more efficient Transformer architectures that still allow long dependencies, of which you can find an overview in the paper by [Tay et al. (2020)](https://arxiv.org/abs/2009.06732) if interested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVS8tKaCnTTD"
      },
      "source": [
        "### Transformer Encoder\n",
        "\n",
        "Next, we will look at how to apply the multi-head attention block inside the Transformer architecture. Originally, the Transformer model was designed for machine translation. Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation. On the other hand, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN. While this structure is extremely useful for Sequence-to-Sequence tasks with the necessity of autoregressive decoding, we will focus here on the encoder part. Many advances in NLP have been made using pure encoder-based Transformer models (if interested, models include the [BERT](https://arxiv.org/abs/1810.04805)-family, the [Vision Transformer](https://arxiv.org/abs/2010.11929), and more), and in our tutorial, we will also mainly focus on the encoder part. If you have understood the encoder architecture, the decoder is a very small step to implement as well. The full Transformer architecture looks as follows (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/transformer_architecture.svg?raw=1\" width=\"400px\"></center>\n",
        "\n",
        "The encoder consists of $N$ identical blocks that are applied in sequence. Taking as input $x$, it is first passed through a Multi-Head Attention block as we have implemented above. The output is added to the original input using a residual connection, and we apply a consecutive Layer Normalization on the sum. Overall, it calculates $\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))$ ($x$ being $Q$, $K$ and $V$ input to the attention layer). The residual connection is crucial in the Transformer architecture for two reasons:\n",
        "\n",
        "1. Similar to ResNets, Transformers are designed to be very deep. Some models contain more than 24 blocks in the encoder. Hence, the residual connections are crucial for enabling a smooth gradient flow through the model.\n",
        "2. Without the residual connection, the information about the original sequence is lost. Remember that the Multi-Head Attention layer ignores the position of elements in a sequence, and can only learn it based on the input features. Removing the residual connections would mean that this information is lost after the first attention layer (after initialization), and with a randomly initialized query and key vector, the output vectors for position $i$ has no relation to its original input. All outputs of the attention are likely to represent similar/same information, and there is no chance for the model to distinguish which information came from which input element. An alternative option to residual connection would be to fix at least one head to focus on its original input, but this is very inefficient and does not have the benefit of the improved gradient flow.\n",
        "\n",
        "The Layer Normalization also plays an important role in the Transformer architecture as it enables faster training and provides small regularization. Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence. We are not using Batch Normalization because it depends on the batch size which is often small with Transformers (they require a lot of GPU memory), and BatchNorm has shown to perform particularly bad in language as the features of words tend to have a much higher variance (there are many, very rare words which need to be considered for a good distribution estimate).\n",
        "\n",
        "Additionally to the Multi-Head Attention, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. Specifically, the model uses a Linear$\\to$ReLU$\\to$Linear MLP. The full transformation including the residual connection can be expressed as:  \n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n",
        "    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "This MLP adds extra complexity to the model and allows transformations on each sequence element separately. You can imagine as this allows the model to \"post-process\" the new information added by the previous Multi-Head Attention, and prepare it for the next attention block. Usually, the inner dimensionality of the MLP is 2-8$\\times$ larger than $d_{\\text{model}}$, i.e. the dimensionality of the original input $x$. The general advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution.\n",
        "\n",
        "Finally, after looking at all parts of the encoder architecture, we can start implementing it below. We first start by implementing a single encoder block. Additionally to the layers described above, we will add dropout layers in the MLP and on the output of the MLP and Multi-Head Attention for regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qX275gCgnTTD"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim - Dimensionality of the input\n",
        "            num_heads - Number of heads to use in the attention block\n",
        "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
        "            dropout - Dropout probability to use in the dropout layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Attention layer\n",
        "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
        "\n",
        "        # Two-layer MLP\n",
        "        self.linear_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, input_dim)\n",
        "        )\n",
        "\n",
        "        # Layers to apply in between the main layers\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Attention part\n",
        "        attn_out = self.self_attn(x, mask=mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # MLP part\n",
        "        linear_out = self.linear_net(x)\n",
        "        x = x + self.dropout(linear_out)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG0SGU_LnTTE"
      },
      "source": [
        "Based on this block, we can implement a module for the full Transformer encoder. Additionally to a forward function that iterates through the sequence of encoder blocks, we also provide a function called `get_attention_maps`. The idea of this function is to return the attention probabilities for all Multi-Head Attention blocks in the encoder. This helps us in understanding, and in a sense, explaining the model. However, the attention probabilities should be interpreted with a grain of salt as it does not necessarily reflect the true interpretation of the model (there is a series of papers about this, including [Attention is not Explanation](https://arxiv.org/abs/1902.10186) and [Attention is not not Explanation](https://arxiv.org/abs/1908.04626))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OLHBhxAvnTTE"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, **block_args):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for l in self.layers:\n",
        "            x = l(x, mask=mask)\n",
        "        return x\n",
        "\n",
        "    def get_attention_maps(self, x, mask=None):\n",
        "        attention_maps = []\n",
        "        for l in self.layers:\n",
        "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
        "            attention_maps.append(attn_map)\n",
        "            x = l(x)\n",
        "        return attention_maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyNhhztEnTTE"
      },
      "source": [
        "### Positional encoding\n",
        "\n",
        "We have discussed before that the Multi-Head Attention block is permutation-equivariant, and cannot distinguish whether an input comes before another one in the sequence or not. In tasks like language understanding, however, the position is important for interpreting the input words. The position information can therefore be added via the input features. We could learn a embedding for every possible position, but this would not generalize to a dynamical input sequence length. Hence, the better option is to use feature patterns that the network can identify from the features and potentially generalize to larger sequences. The specific pattern chosen by Vaswani et al. are sine and cosine functions of different frequencies, as follows:\n",
        "\n",
        "$$\n",
        "PE_{(pos,i)} = \\begin{cases}\n",
        "    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n",
        "    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$PE_{(pos,i)}$ represents the position encoding at position $pos$ in the sequence, and hidden dimensionality $i$. These values, concatenated for all hidden dimensions, are added to the original input features (in the Transformer visualization above, see \"Positional encoding\"), and constitute the position information. We distinguish between even ($i \\text{ mod } 2=0$) and uneven ($i \\text{ mod } 2=1$) hidden dimensionalities where we apply a sine/cosine respectively. The intuition behind this encoding is that you can represent $PE_{(pos+k,:)}$ as a linear function of $PE_{(pos,:)}$, which might allow the model to easily attend to relative positions. The wavelengths in different dimensions range from $2\\pi$ to $10000\\cdot 2\\pi$.\n",
        "\n",
        "The positional encoding is implemented below. The code is taken from the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model) about Transformers on NLP and adjusted for our purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aCju5launTTE"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            d_model - Hidden dimensionality of the input.\n",
        "            max_len - Maximum length of a sequence to expect.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
        "        # Used for tensors that need to be on the same device as the module.\n",
        "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
        "        self.register_buffer('pe', pe, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD0XTm8ZnTTE"
      },
      "source": [
        "To understand the positional encoding, we can visualize it below. We will generate an image of the positional encoding over hidden dimensionality and position in a sequence. Each pixel, therefore, represents the change of the input feature we perform to encode the specific position. Let's do it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "sW3jHjPmnTTF",
        "outputId": "15c01a14-0086-4ec3-89ea-8718c277acbb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"448.724438pt\" height=\"226.194375pt\" viewBox=\"0 0 448.724438 226.194375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-02-26T18:12:33.862468</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 226.194375 \nL 448.724438 226.194375 \nL 448.724438 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 188.638125 \nL 373.243125 188.638125 \nL 373.243125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#p80d2ac24c9)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image932d68f967\" transform=\"scale(1 -1) translate(0 -166.32)\" x=\"40.603125\" y=\"-22.318125\" width=\"332.64\" height=\"166.32\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"mb6cdde407a\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mb6cdde407a\" x=\"40.603125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 1 -->\n      <g transform=\"translate(37.421875 203.236563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#mb6cdde407a\" x=\"71.788125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g transform=\"translate(65.425625 203.236563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#mb6cdde407a\" x=\"106.438125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g transform=\"translate(100.075625 203.236563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mb6cdde407a\" x=\"141.088125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <g transform=\"translate(134.725625 203.236563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#mb6cdde407a\" x=\"175.738125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <g transform=\"translate(169.375625 203.236563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mb6cdde407a\" x=\"210.388125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <g transform=\"translate(204.025625 203.236563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#mb6cdde407a\" x=\"245.038125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <g transform=\"translate(238.675625 203.236563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mb6cdde407a\" x=\"279.688125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 70 -->\n      <g transform=\"translate(273.325625 203.236563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#mb6cdde407a\" x=\"314.338125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 80 -->\n      <g transform=\"translate(307.975625 203.236563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#mb6cdde407a\" x=\"348.988125\" y=\"188.638125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 90 -->\n      <g transform=\"translate(342.625625 203.236563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-39\" d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- Position in sequence -->\n     <g transform=\"translate(155.627813 216.914688) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \nL 1259 2394 \nL 2053 2394 \nQ 2494 2394 2734 2622 \nQ 2975 2850 2975 3272 \nQ 2975 3691 2734 3919 \nQ 2494 4147 2053 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 2053 4666 \nQ 2838 4666 3239 4311 \nQ 3641 3956 3641 3272 \nQ 3641 2581 3239 2228 \nQ 2838 1875 2053 1875 \nL 1259 1875 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\nM 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 -1331 \nL 2906 -1331 \nL 2906 525 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <defs>\n       <path id=\"m1d2b395135\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m1d2b395135\" x=\"40.603125\" y=\"22.318125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1 -->\n      <g transform=\"translate(27.240625 26.117344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m1d2b395135\" x=\"40.603125\" y=\"53.503125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 57.302344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#m1d2b395135\" x=\"40.603125\" y=\"88.153125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 91.952344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m1d2b395135\" x=\"40.603125\" y=\"122.803125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 126.602344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#m1d2b395135\" x=\"40.603125\" y=\"157.453125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 161.252344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- Hidden dimension -->\n     <g transform=\"translate(14.798438 150.710938) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-48\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 2753 \nL 3553 2753 \nL 3553 4666 \nL 4184 4666 \nL 4184 0 \nL 3553 0 \nL 3553 2222 \nL 1259 2222 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-48\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"75.195312\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"102.978516\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"166.455078\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"229.931641\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"291.455078\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"354.833984\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"386.621094\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"450.097656\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"477.880859\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"575.292969\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"636.816406\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"700.195312\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"752.294922\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"780.078125\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"841.259766\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 188.638125 \nL 40.603125 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 373.243125 188.638125 \nL 373.243125 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 188.638125 \nL 373.243125 188.638125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 373.243125 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_18\">\n    <!-- Positional encoding over hidden dimensions -->\n    <g transform=\"translate(74.873437 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-50\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n     <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n     <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n     <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"969.324219\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"1001.111328\"/>\n     <use xlink:href=\"#DejaVuSans-76\" x=\"1062.292969\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1121.472656\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"1182.996094\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"1224.109375\"/>\n     <use xlink:href=\"#DejaVuSans-68\" x=\"1255.896484\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1319.275391\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"1347.058594\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"1410.535156\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1474.011719\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1535.535156\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"1598.914062\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"1630.701172\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1694.177734\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"1721.960938\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1819.373047\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1880.896484\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"1944.275391\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1996.375\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"2024.158203\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"2085.339844\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"2148.71875\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 395.563125 188.638125 \nL 403.879125 188.638125 \nL 403.879125 22.318125 \nL 395.563125 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAAwAAADnCAYAAAAq9remAAABLElEQVR4nO2YSw6DMBBDZ5KQa/R4vf+aX6/Qh2RkQrrGsj1vElDzG58zwK/VJI9HtJpM4ejQC3aAgoIeH2Sshg4dgnDsMIJDwZEguZYVC2gHKsgCI3EHLlBHqktlggsdMDj5atAXip4D7lA7BUdL6znwS4AfUf29RCMti9qBTklfGoO7gYN++XpTO1R8e7+QNI/UOhMELw3BRVEL8JQcI1FwFziop5R8rPg86JePRlrwifNb78QOVJDybcUdqCAr/Py5ocMNU/IDx8+DGlxOcH8I0B/GEdGO44AOJ/PgAsNIhh1GcDAUGJbe913sMMJYDR304AxLj+AwwT3UYds2scMUeAgm6YcKRgCH3w8jdNCDMxyrYSQsWNdV7DDCWA0jvfJufaVggnuowA/cD++JW/ZYqqiKAAAAAElFTkSuQmCC\" id=\"imageae59be2b03\" transform=\"scale(1 -1) translate(0 -166.32)\" x=\"395.28\" y=\"-22.32\" width=\"8.64\" height=\"166.32\"/>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_6\">\n     <g id=\"line2d_16\">\n      <defs>\n       <path id=\"m9f8c8a89d5\" d=\"M 0 0 \nL 3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m9f8c8a89d5\" x=\"403.879125\" y=\"167.848836\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- −0.75 -->\n      <g transform=\"translate(410.879125 171.648055) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"179.199219\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_17\">\n      <g>\n       <use xlink:href=\"#m9f8c8a89d5\" x=\"403.879125\" y=\"147.058735\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- −0.50 -->\n      <g transform=\"translate(410.879125 150.857953) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"242.822266\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m9f8c8a89d5\" x=\"403.879125\" y=\"126.268633\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- −0.25 -->\n      <g transform=\"translate(410.879125 130.067852) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"179.199219\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"242.822266\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_19\">\n      <g>\n       <use xlink:href=\"#m9f8c8a89d5\" x=\"403.879125\" y=\"105.478531\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 0.00 -->\n      <g transform=\"translate(410.879125 109.27775) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#m9f8c8a89d5\" x=\"403.879125\" y=\"84.68843\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 0.25 -->\n      <g transform=\"translate(410.879125 88.487649) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_21\">\n      <g>\n       <use xlink:href=\"#m9f8c8a89d5\" x=\"403.879125\" y=\"63.898328\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 0.50 -->\n      <g transform=\"translate(410.879125 67.697547) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#m9f8c8a89d5\" x=\"403.879125\" y=\"43.108227\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 0.75 -->\n      <g transform=\"translate(410.879125 46.907445) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_23\">\n      <g>\n       <use xlink:href=\"#m9f8c8a89d5\" x=\"403.879125\" y=\"22.318125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 1.00 -->\n      <g transform=\"translate(410.879125 26.117344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\"/>\n   <g id=\"patch_8\">\n    <path d=\"M 395.563125 188.638125 \nL 399.721125 188.638125 \nL 403.879125 188.638125 \nL 403.879125 22.318125 \nL 399.721125 22.318125 \nL 395.563125 22.318125 \nL 395.563125 188.638125 \nz\n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p80d2ac24c9\">\n   <rect x=\"40.603125\" y=\"22.318125\" width=\"332.64\" height=\"166.32\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "application/pdf": "JVBERi0xLjQKJazcIKu6CjEgMCBvYmoKPDwgL1R5cGUgL0NhdGFsb2cgL1BhZ2VzIDIgMCBSID4+CmVuZG9iago4IDAgb2JqCjw8IC9Gb250IDMgMCBSIC9YT2JqZWN0IDcgMCBSIC9FeHRHU3RhdGUgNCAwIFIgL1BhdHRlcm4gNSAwIFIKL1NoYWRpbmcgNiAwIFIgL1Byb2NTZXQgWyAvUERGIC9UZXh0IC9JbWFnZUIgL0ltYWdlQyAvSW1hZ2VJIF0gPj4KZW5kb2JqCjExIDAgb2JqCjw8IC9UeXBlIC9QYWdlIC9QYXJlbnQgMiAwIFIgL1Jlc291cmNlcyA4IDAgUgovTWVkaWFCb3ggWyAwIDAgNDQ4LjcwNzI1IDIyNi4xODg3NSBdIC9Db250ZW50cyA5IDAgUiAvQW5ub3RzIDEwIDAgUiA+PgplbmRvYmoKOSAwIG9iago8PCAvTGVuZ3RoIDEyIDAgUiAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJytV01z2zYQveNX4JgcvML3Akc7jt3mVNea5NDpwWMxsjwWNYrq+O93QUkkSMIgM5MDPeaS3PfeArt4WlxXPzeP1d+3V/zTPVt0d48HJvkzXWsu+DNdb1zyW7rWTNDdlhnjAQUqS3cvyZ1SDqT3aCkqendPjH1ni0tKcaBvbhkzAmwQjj7SCNZoemvLNGpQuh99SaNKaPDuGO4ypNEGaM8z6bVW4AyXzoFW/EfFv/GaLy5VpERy6XqL1Hi/EHv6ToN2PCo/fZtJ/rjliz8lv97xO3bH9+e8gkoYcwvwp+wUyWtPggbESTq7olV4Y3v6K/gFEYhfGClCfKwUhIjMrpZscSO5FHz5vVmh5Yr9wz/Ij/xfvvzCPi/ZHWsYMJSAaAfISbCE7CwhN+WewhVjYCkcGDVETqMlaCkECDcHW+WwjQQxUp1Gi9j0FGfp1jlstIBj3Um0iO0Cbb052CaDraQAPdKdRkvYStDTWbptDttYECPdabSIrT24WbpdDhsDuLHuJFrEpmmjZ+nGDLaWhibEEDuNFptbIIRZun0Om+ZwGOlOo0VsQ+Nxlu7Qw063q7Vg0bg48gMo9FKV8vz1MTKKfD7sDpv/Nruab2p+qPavVf1YjfXlBn48M0CMj4HcutIhIpvXmmHi8RdnZwcvkUqFfgDfRfPtBB5lfCqdB+/lBH5mfRMC1B9CjQi00QkCVHUqRZlAbpAmBKjUelyBNlomEALYqQLkhmmHTyXEkf42WEZ3BsKUejPY4jHDRcxFjYzN4UsT3BgbdDHNH5vVqqr5arOt6gPt7zSp4l+O7qqxBn3TMWEQehv9Puuatu+5Jnr/F6xX7+1+172XXTS61omlWqfrgAa8OLZhOH2W1E+NJ4QF2Qyv84x4eOE0HnarTb3mu5/VD/40KPIhrfLJcqZWVpysLNOBxpUdlznKcYNCJNFUb5JiZEJnWEsfDWliLGM65Y+4/uwq1dFVJo7y7CbXKS3rgZyq0KHRgNE39MO5riAr2X5vwGo6ADQKF0Z72sfGPm6D8+IIQNuVek+shGzEtP9E+jfy4rp6fvj6ev9QHy62m/r1cHLJbVO3EuiEljSYTF9BG50QQB2plVPOaiXUPAGpc/kdAmiwBamVUX0FXXhCQqAfTLQCzgWtx3MlK0H95jWQNChRaG0Gq5DEJ0SQAiCWZAMsXYXpSByzQ77jQhbdBIK0Ay5dfIoLtZILziI5WSpIiUtayBwXp6g1tTU44NLFp7hQKxr6HyWiNkUuOUedcvGafrpoZ/yASxef4oLRnzkior3DIhcs1yU9FBIqZS+WMJnnxgY7pXR2vjPWNTUhCv/+WJ8z7NsUM86AHI3mZLgnAf8DboyWgQplbmRzdHJlYW0KZW5kb2JqCjEyIDAgb2JqCjEwMjEKZW5kb2JqCjEwIDAgb2JqClsgXQplbmRvYmoKMTkgMCBvYmoKPDwgL0xlbmd0aCA3OSAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJwzNzVSMFCwtAASZqYmCuZGlgophlxAPoiVy2VoaQ5m5YBZJsYGQJapqSkSCyIL0wthweRgtLGJOdQEBAskB7Y2B2ZbDlcGVxoA1pQcDAplbmRzdHJlYW0KZW5kb2JqCjIwIDAgb2JqCjw8IC9MZW5ndGggMTcwIC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nD2QSxLDIAxD95xCRwD/gPO00+mC3H9by5l0gxRjyy9EV3TslYfHxpSN92hjT4QtXOV0Gk5TGY+Lu2ZdoMthMtNvvJq5wFRhkdXsovoYvKHzrGaHr1UzMYQ3mRIaYCp3cg/19ac47duSkGxXYdCdGqSzMMyR/D0QU3PQc4iR/CNfcmth0JnmFxctqxmtZUzR7GGqbC0M6o1Bd8r11Hqu8zAR7/MD30E+ZAplbmRzdHJlYW0KZW5kb2JqCjIxIDAgb2JqCjw8IC9MZW5ndGggMzA3IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nD2SS24DMQxD9z6FLhDA+tme86Qoupjef9snJemKHNkWRWqWukxZUx6QNJOEf+nwcLGd8jtsz2Zm4Fqil4nllOfQFWLuonzZzEZdWSfF6oRmOrfoUTkXBzZNqp+rLKXdLngO1yaeW/YRP7zQoB7UNS4JN3RXo2UpNGOq+3/Se/yMMuBqTF1sUqt7HzxeRFXo6AdHiSJjlxfn40EJ6UrCaFqIlXdFA0Hu8rTKewnu295qyLIHqZjOOylmsOt0Ui5uF4chHsjyqPDlo9hrQs/4sCsl9EjYhjNyJ+5oxubUyOKQ/t6NBEuPrmgh8+CvbtYuYLxTOkViZE5yrGmLVU73UBTTucO9DBD1bEVDKXOR1epfw84La5ZsFnhK+gUeo90mSw5W2duoTu+tPNnQ9x9a13QfCmVuZHN0cmVhbQplbmRvYmoKMjIgMCBvYmoKPDwgL0xlbmd0aCAyMzIgL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicNVFJbsQwDLv7FfzAANbuvCfFoIf2/9dSyhQIQCW2uCViYyMCLzH4OYjc+JI1oyZ+Z3JX/CxPhUfCreBJFIGX4V52gssbxmU/DjMfvJdWzqTGkwzIRTY9PBEy2CUQOjC7BnXYZtqJviHhsyNSzUaW09cS9NIqBMpTtt/pghJtq/pz+6wLbfvaE052e+pJ5ROI55aswGXjFZPFWAY9UblLMX2Q6myhJ6G8KJ+DbD5qiESXKGfgicHBKNAO7LntZ+JVIWhd3adtY6hGSsfTvw1NTZII+UQJZ7Y07hb+f8+9vtf7D04hVBEKZW5kc3RyZWFtCmVuZG9iagoyMyAwIG9iago8PCAvTGVuZ3RoIDIzMSAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJw1TzmSBCEMy3mFPjBVGNtAv6entjbY+X+6kplOkPAhydMTHZl4mSMjsGbH21pkIGbgU0zFv/a0DxOq9+AeIpSLC2GGkXDWrONuno4X/3aVz1gH7zb4illeENjCTNZXFmcu2wVjaZzEOclujF0TsY11radTWEcwoQyEdLbDlCBzVKT0yY4y5ug4kSeei+/22yx2OX4O6ws2jSEV5/gqeoI2g6Lsee8CGnJB/13d+B5Fu+glIBsJFtZRYu6c5YRfvXZ0HrUoEnNCmkEuEyHN6SqmEJpQrLOjoFJRcKk+p+isn3/lX1wtCmVuZHN0cmVhbQplbmRvYmoKMjQgMCBvYmoKPDwgL0xlbmd0aCAyNDkgL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicPVA7jkQhDOs5hS/wJPIjcB5Gqy1m79+uA5opUEx+tjMk0BGBRwwxlK/jJa2groG/i0LxbuLrg8Igq0NSIM56D4h07KY2kRM6HZwzP2E3Y47ARTEGnOl0pj0HJjn7wgqEcxtl7FZIJ4mqIo7qM44pnip7n3gWLO3INlsnkj3kIOFSUonJpZ+Uyj9typQKOmbRBCwSueBkE004y7tJUowZlDLqHqZ2In2sPMijOuhkTc6sI5nZ00/bmfgccLdf2mROlcd0Hsz4nLTOgzkVuvfjiTYHTY3a6Oz3E2kqL1K7HVqdfnUSld0Y5xgSl2d/Gd9k//kH/odaIgplbmRzdHJlYW0KZW5kb2JqCjI1IDAgb2JqCjw8IC9MZW5ndGggMzk1IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nD1SS27FQAjb5xRcoNLwm895UlXdvPtva0NSqSq8iTHGMH3KkLnlS10ScYXJt16uWzymfC5bWpl5iLuLjSU+ttyX7iG2XXQusTgdR/ILMp0qRKjNqtGh+EKWhQeQTvChC8J9Of7jL4DB17ANuOE9MkGwJOYpQsZuURmaEkERYeeRFaikUJ9Zwt9R7uv3MgVqb4ylC2Mc9Am0BUJtSMQC6kAAROyUVK2QjmckE78V3WdiHGDn0bIBrhlURJZ77MeIqc6ojLxExD5PTfoolkwtVsZuUxlf/JSM1Hx0BSqpNPKU8tBVs9ALWIl5EvY5/Ej459ZsIYY6btbyieUfM8UyEs5gSzlgoZfjR+DbWXURrh25uM50gR+V1nBMtOt+yPVP/nTbWs11vHIIokDlTUHwuw6uRrHExDI+nY0peqIssBqavEYzwWEQEdb3w8gDGv1yvBA0p2sitFgim7ViRI2KbHM9vQTWTO/FOdbDE8Js753WobIzMyohgtq6hmrrQHazvvNwtp8/M+iibQplbmRzdHJlYW0KZW5kb2JqCjI2IDAgb2JqCjw8IC9MZW5ndGggMjQ5IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nE1RSYoDMAy75xX6QCFek7ynQ5lD5//Xyg6FOQQJr5KTlphYCw8xhB8sPfiRIXM3/Rt+otm7WXqSydn/mOciU1H4UqguYkJdiBvPoRHwPaFrElmxvfE5LKOZc74HH4W4BDOhAWN9STK5qOaVIRNODHUcDlqkwrhrYsPiWtE8jdxu+0ZmZSaEDY9kQtwYgIgg6wKyGCyUNjYTMlnOA+0NyQ1aYNepG1GLgiuU1gl0olbEqszgs+bWdjdDLfLgqH3x+mhWl2CF0Uv1WHhfhT6YqZl27pJCeuFNOyLMHgqkMjstK7V7xOpugfo/y1Lw/cn3+B2vD838XJwKZW5kc3RyZWFtCmVuZG9iagoyNyAwIG9iago8PCAvTGVuZ3RoIDk0IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nEWNwRHAIAgE/1RBCQoK2k8mk4f2/40QMnxg5w7uhAULtnlGHwWVJl4VWAdKY9xQj0C94XItydwFD3Anf9rQVJyW03dpkUlVKdykEnn/DmcmkKh50WOd9wtj+yM8CmVuZHN0cmVhbQplbmRvYmoKMjggMCBvYmoKPDwgL0xlbmd0aCAzNDEgL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicRVJLbkQxCNu/U3CBSOGXkPO0qrqY3n9bm0zVzeAJYGx4y1OmZMqwuSUjJNeUT30iQ6ym/DRyJCKm+EkJBXaVj8drS6yN7JGoFJ/a8eOx9Eam2RVa9e7Rpc2iUc3KyDnIEKGeFbqye9QO2fB6XEi675TNIRzL/1CBLGXdcgolQVvQd+wR3w8droIrgmGway6D7WUy1P/6hxZc7333YscugBas577BDgCopxO0BcgZ2u42KWgAVbqLScKj8npudqJso1Xp+RwAMw4wcsCIJVsdvtHeAJZ9XehFjYr9K0BRWUD8yNV2wd4xyUhwFuYGjr1wPMWZcEs4xgJAir3iGHrwJdjmL1euiJrwCXW6ZC+8wp7a5udCkwh3rQAOXmTDraujqJbt6TyC9mdFckaM1Is4OiGSWtI5guLSoB5a41w3seJtI7G5V9/uH+GcL1z26xdL7ITECmVuZHN0cmVhbQplbmRvYmoKMjkgMCBvYmoKPDwgL0xlbmd0aCAxNjQgL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicRZDHcQUxDEPvqgIlMIAK9azH8w/r/q+G9NNBehhCDGJPwrBcV3FhdMOPty0zDX9HGe7G+jJjvNVYICfoAwyRiavRpPp2xRmq9OTVYq6jolwvOiISzJLjq0AjfDqyx5O2tjP9dF4f7CHvE/8qKuduYQEuqu5A+VIf8dSP2VHqmqGPKitrHmraV4RdEUrbPi6nMk7dvQNa4b2Vqz3a7z8edjryCmVuZHN0cmVhbQplbmRvYmoKMzAgMCBvYmoKPDwgL0xlbmd0aCA3MiAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJwzMrdQMFCwNAEShhYmCuZmBgophlxAvqmJuUIuF0gMxMoBswyAtCWcgohngJggbRDFIBZEsZmJGUQdnAGRy+BKAwAl2xbJCmVuZHN0cmVhbQplbmRvYmoKMzEgMCBvYmoKPDwgL0xlbmd0aCA0NyAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJwzMrdQMFCwNAEShhYmCuZmBgophlyWEFYuF0wsB8wC0ZZwCiKewZUGALlnDScKZW5kc3RyZWFtCmVuZG9iagozMiAwIG9iago8PCAvTGVuZ3RoIDI1OCAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJxFkUtyBCAIRPeegiOA/OQ8k0plMbn/Ng3OZDZ2l6j9hEojphIs5xR5MH3J8s1ktul3OVY7GwUURSiYyVXosQKrO1PEmWuJautjZeS40zsGxRvOXTmpZHGjjHVUdSpwTM+V9VHd+XZZlH1HDmUK2KxzHGzgym3DGCdGm63uDveJIE8nU0fF7SDZ8AcnjX2VqytwnWz20UswDgT9QhOY5ItA6wyBxs1T9OQS7OPjdueBYG95EUjZEMiRIRgdgnadXP/i1vm9/3GGO8+1Ga4c7+J3mNZ2x19ikhVzAYvcKajnay5a1xk63pMzx+Sm+4bOuWCXu4NM7/k/1s/6/gMeKWb6CmVuZHN0cmVhbQplbmRvYmoKMzMgMCBvYmoKPDwgL1R5cGUgL1hPYmplY3QgL1N1YnR5cGUgL0Zvcm0gL0JCb3ggWyAtMTAyMSAtNDYzIDE3OTQgMTIzMyBdIC9MZW5ndGggMzkKL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnic4zI0MFMwNjVVyOUyNzYCs3LALCNzIyALJItgQWQzuNIAFfMKfAplbmRzdHJlYW0KZW5kb2JqCjM0IDAgb2JqCjw8IC9MZW5ndGggMTYzIC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nEWQOxIDIQxDe06hI/gjAz7PZjIpNvdvY9hsUsDTWCCDuxOC1NqCieiCh7Yl3QXvrQRnY/zpNm41EuQEdYBWpONolFJ9ucVplXTxaDZzKwutEx1mDnqUoxmgEDoV3u2i5HKm7s75Q3D1X/W/Yt05m4mBycodCM3qU9z5NjuiurrJ/qTH3KzXfivsVWFpWUvLCbedu2ZACdxTOdqrPT8fCjr2CmVuZHN0cmVhbQplbmRvYmoKMzUgMCBvYmoKPDwgL0xlbmd0aCAzMjIgL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicNVG7bcUwDOw1BRcwIH4lzeMgSJG3f5s72qlI07wfVV4ypVwudckqWWHypUN1iqZ8nmam/A71kOOYHtkhulPWlnsYFpaJeUodsZos93ALNr4AmhJzC/H3CPArgFHARKBu8fcPulkSQBoU/BTomquWWGICDYuFrdkV4lbdKVi4q/h2JLkHCXIxWehTDkWKKbfAfBks2ZFanOtyWQr/bn0CGmGFOOyzi0TgecADTCT+ZIBszz5b7OrqRTZ2hjjp0ICLgJvNJAFBUzirPrhh+2q75ueZKCc4OdavojG+DU7mS1LeV7nHz6BB3vgzPGd3jlAOmlAI9N0CIIfdwEaEPrXPwC4Dtkm7d2NK+ZxkKb4ENgr2qFMdyvBi7MxWb9j8x+jKZlFskJX10ekOytygE2Ieb2ShW7K2+zcPs33/AV8Ze2QKZW5kc3RyZWFtCmVuZG9iagozNiAwIG9iago8PCAvTGVuZ3RoIDIxOCAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJw9ULmNBDEMy12FGljAeu2pZxaLS6b/9Ej59iLRFkVSKjWZkikvdZQlWVPeOnyWxA55huVuZDYlKkUvk7Al99AK8X2J5hT33dWWs0M0l2g5fgszKqobHdNLNppwKhO6oNzDM/oNbXQDVocesVsg0KRg17YgcscPGAzBmROLIgxKTQb/rnKPn16LGz7D8UMUkZIO5jX/WP3ycw2vU48nkW5vvuJenKkOAxEckpq8I11YsS4SEWk1QU3PwFotgLu3Xv4btCO6DED2icRxmlKOob9rcKXPL+UnU9gKZW5kc3RyZWFtCmVuZG9iagozNyAwIG9iago8PCAvTGVuZ3RoIDgzIC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nEWMuw3AMAhEe6ZgBH4m9j5RlMLevw0QJW64J909XB0JmSluM8NDBp4MLIZdcYH0ljALXEdQjp3so2HVvuoEjfWmUvPvD5Se7KzihusBAkIaZgplbmRzdHJlYW0KZW5kb2JqCjM4IDAgb2JqCjw8IC9MZW5ndGggNTEgL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicMza0UDBQMDQwB5JGhkCWkYlCiiEXSADEzOWCCeaAWQZAGqI4B64mhyuDKw0A4bQNmAplbmRzdHJlYW0KZW5kb2JqCjM5IDAgb2JqCjw8IC9MZW5ndGggMjQzIC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nE1Ru60DMQzrPYUWOMD62b55Lnh4xWX/NqScBKlEQxRJycNTumTKYX1KRkiOLg9tGktsujw3QlOHioKpa4nqlKuZpsxTLE3Q895ZruYY4HtVN9Tf9IheApFRglVhgQ6QO7hg+NlrJmxRCyIxhlAzgGnCCnO4EjEEGYy1ZxiUKgxO1c8qV/svp2XYKrB4MJ0iP7KaaKdfuhx46ykHQtjclbt6IU0I7o0GY8wsXHepsp0AHEx0mYmMWLwNx9MhDA1emgascNaNmCCxGyOlD14HGdOwd0UedbcY8b5bxpS71c99UX3mXe0fCMEbJ/h7AcobXV4KZW5kc3RyZWFtCmVuZG9iago0MCAwIG9iago8PCAvTGVuZ3RoIDE2MCAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJxFkDkSAzEIBHO9gidIXIL3rMu1wfr/qQfWR6LpAjQcuhZNynoUaD7psUahutBr6CxKkkTBFpIdUKdjiDsoSExIY5JIth6DI5pYs12YmVQqs1LhtGnFwr/ZWtXIRI1wjfyJ6QZU/E/qXJTwTYOvkjH6GFS8O4OMSfheRdxaMe3+RDCxGfYJb0UmBYSJsanZvs9ghsz3Ctc4x/MNTII36wplbmRzdHJlYW0KZW5kb2JqCjQxIDAgb2JqCjw8IC9MZW5ndGggMzM0IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nC1SS3LFIAzbcwpdoDP4B+Q86XS6eL3/tpKTRUYOYPQx5YaJSnxZILej1sS3jcxAheGvq8yFz0jbyDqIy5CLuJIthXtELOQxxDzEgu+r8R4e+azMybMHxi/Zdw8r9tSEZSHjxRnaYRXHYRXkWLB1Iap7eFOkw6kk2OOL/z7Fcy0ELXxG0IBf5J+vjuD5khZp95ht0656sEw7qqSwHGxPc14mX1pnuToezwfJ9q7YEVK7AhSFuTPOc+Eo01ZGtBZ2NkhqXGxvjv1YStCFblxGiiOQn6kiPKCkycwmCuKPnB5yKgNh6pqudHIbVXGnnsw1m4u3M0lm675IsZnCeV04s/4MU2a1eSfPcqLUqQjvsWdL0NA5rp69lllodJsTvKSEz8ZOT06+VzPrITkVCaliWlfBaRSZYgnbEl9TUVOaehn++/Lu8Tt+/gEsc3xzCmVuZHN0cmVhbQplbmRvYmoKNDIgMCBvYmoKPDwgL0xlbmd0aCA3MCAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJwzMzZTMFCwMAISpqaGCuZGlgophlxAPoiVywUTywGzzCzMgSwjC5CWHC5DC2MwbWJspGBmYgZkWSAxILoyuNIAmJoTAwplbmRzdHJlYW0KZW5kb2JqCjQzIDAgb2JqCjw8IC9MZW5ndGggMzIwIC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nDVSS24FMQjbzym4QKXwT87zqqqLvvtvaxO9FUwwYOMpL1nSS77UJdulw+RbH/clsULej+2azFLF9xazFM8tr0fPEbctCgRREz1YmS8VItTP9Og6qHBKn4FXCLcUG7yDSQCDavgHHqUzIFDnQMa7YjJSA4Ik2HNpcQiJciaJf6S8nt8nraSh9D1Zmcvfk0ul0B1NTugBxcrFSaBdSfmgmZhKRJKX632xQvSGwJI8PkcxyYDsNoltogUm5x6lJczEFDqwxwK8ZprVVehgwh6HKYxXC7OoHmzyWxOVpB2t4xnZMN7LMFNioeGwBdTmYmWC7uXjNa/CiO1Rk13DcO6WzXcI0Wj+GxbK4GMVkoBHp7ESDWk4wIjAnl44xV7zEzkOwIhjnZosDGNoJqd6jonA0J6zpWHGxx5a9fMPVOl8hwplbmRzdHJlYW0KZW5kb2JqCjQ0IDAgb2JqCjw8IC9MZW5ndGggMTggL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicMza0UDCAwxRDrjQAHeYDUgplbmRzdHJlYW0KZW5kb2JqCjQ1IDAgb2JqCjw8IC9MZW5ndGggMTMzIC9GaWx0ZXIgL0ZsYXRlRGVjb2RlID4+CnN0cmVhbQp4nEWPSw4EIQhE95yijsDHH+dxMumFc//tgJ1uE2M9hVSBuYKhPS5rA50VHyEZtvG3qZaORVk+VHpSVg/J4Iesxssh3KAs8IJJKoYhUIuYGpEtZW63gNs2DbKylVOljrCLozCP9rRsFR5folsidZI/g8QqL9zjuh3Ipda73qKLvn+kATEJCmVuZHN0cmVhbQplbmRvYmoKNDYgMCBvYmoKPDwgL0xlbmd0aCAzNDAgL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicNVI5bgQxDOv9Cn0ggG7b79kgSJH8vw2p2RQDcXRSlDtaVHbLh4VUtex0+bSV2hI35HdlhcQJyasS7VKGSKi8ViHV75kyr7c1ZwTIUqXC5KTkccmCP8OlpwvH+baxr+XIHY8eWBUjoUTAMsXE6BqWzu6wZlt+lmnAj3iEnCvWLcdYBVIb3TjtiveheS2yBoi9mZaKCh1WiRZ+QfGgR4199hhUWCDR7RxJcIyJUJGAdoHaSAw5eyx2UR/0MygxE+jaG0XcQYElkpg5xbp09N/40LGg/tiMN786KulbWllj0j4b7ZTGLDLpelj0dPPWx4MLNO+i/OfVDBI0ZY2Sxget2jmGoplRVni3Q5MNzTHHIfMOnsMZCUr6PBS/jyUTHZTI3w4NoX9fHqOMnDbeAuaiP20VBw7is8NeuYEVShdrkvcBqUzogen/r/G1vtfXHx3tgMYKZW5kc3RyZWFtCmVuZG9iago0NyAwIG9iago8PCAvTGVuZ3RoIDI1MSAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJwtUUlyA0EIu88r9IRmp99jlyuH5P/XCMoHBg2LQHRa4qCMnyAsV7zlkatow98zMYLfBYd+K9dtWORAVCBJY1A1oXbxevQe2HGYCcyT1rAMZqwP/Iwp3OjF4TEZZ7fXZdQQ7F2vPZlByaxcxCUTF0zVYSNnDj+ZMi60cz03IOdGWJdhkG5WGjMSjjSFSCGFqpukzgRBEoyuRo02chT7pS+PdIZVjagx7HMtbV/PTThr0OxYrPLklB5dcS4nFy+sHPT1NgMXUWms8kBIwP1uD/VzspPfeEvnzhbT43vNyfLCVGDFm9duQDbV4t+8iOP7jK/n5/n8A19gW4gKZW5kc3RyZWFtCmVuZG9iago0OCAwIG9iago8PCAvTGVuZ3RoIDE3NCAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJxNkEkOQyEMQ/ecwheohDPA5zy/qrpo77+tQwd1gfzkIHA8PNBxJC50ZOiMjiubHOPAsyBj4tE4/8m4PsQxQd2iLViXdsfZzBJzwjIxArZGydk8osAPx1wIEmSXH77AICJdj/lW81mT9M+3O92PurRmXz2iwInsCMWwAVeA/brHgUvC+V7T5JcqJWMTh/KB6iJSNjuhELVU7HKqirPdmytwFfT80UPu7QW1IzzfCmVuZHN0cmVhbQplbmRvYmoKNDkgMCBvYmoKPDwgL0xlbmd0aCA3NSAvRmlsdGVyIC9GbGF0ZURlY29kZSA+PgpzdHJlYW0KeJwztTRSMFAwNgASpmZGCqYm5gophlxAPoiVy2VoZApm5XAZWZopWFgAGSZm5lAhmIYcLmNTc6ABQEXGpmAaqj+HK4MrDQCVkBLvCmVuZHN0cmVhbQplbmRvYmoKNTAgMCBvYmoKPDwgL0xlbmd0aCAyMTUgL0ZpbHRlciAvRmxhdGVEZWNvZGUgPj4Kc3RyZWFtCnicNVE5DgMhDOz3Ff5AJIwveE+iKM3+v82M0VYewVyGtJQhmfJSk6gh5VM+epkunLrc18xqNOeWtC1zgLi2vC+tksCJZoiDwWmYuAGaPAFD19GoUUMXHtDUpVMosNwEPoq3bg/dY7WBl7Yh54kgYigZLEHNqUUTFm3PJ6Q1v16LG96X7d3IU6XGlhiBBgFWOBzX6NfwlT1PJtF0FTLUqzXLGAkTRSI8+Y6m1RPrWjTSMhLUxhGsagO8O/0wTgAAE3HLAmSfSpSz5MRvsfSzBlf6/gGfR1SWCmVuZHN0cmVhbQplbmRvYmoKMTcgMCBvYmoKPDwgL1R5cGUgL0ZvbnQgL0Jhc2VGb250IC9CTVFRRFYrRGVqYVZ1U2FucyAvRmlyc3RDaGFyIDAgL0xhc3RDaGFyIDI1NQovRm9udERlc2NyaXB0b3IgMTYgMCBSIC9TdWJ0eXBlIC9UeXBlMyAvTmFtZSAvQk1RUURWK0RlamFWdVNhbnMKL0ZvbnRCQm94IFsgLTEwMjEgLTQ2MyAxNzk0IDEyMzMgXSAvRm9udE1hdHJpeCBbIDAuMDAxIDAgMCAwLjAwMSAwIDAgXQovQ2hhclByb2NzIDE4IDAgUgovRW5jb2RpbmcgPDwgL1R5cGUgL0VuY29kaW5nCi9EaWZmZXJlbmNlcyBbIDMyIC9zcGFjZSA0NiAvcGVyaW9kIDQ4IC96ZXJvIC9vbmUgL3R3byAvdGhyZWUgL2ZvdXIgL2ZpdmUgL3NpeCAvc2V2ZW4KL2VpZ2h0IC9uaW5lIDcyIC9IIDgwIC9QIDk3IC9hIDk5IC9jIC9kIC9lIDEwMyAvZyAvaCAvaSAxMDggL2wgL20gL24gL28gMTEzCi9xIC9yIC9zIC90IC91IC92IF0KPj4KL1dpZHRocyAxNSAwIFIgPj4KZW5kb2JqCjE2IDAgb2JqCjw8IC9UeXBlIC9Gb250RGVzY3JpcHRvciAvRm9udE5hbWUgL0JNUVFEVitEZWphVnVTYW5zIC9GbGFncyAzMgovRm9udEJCb3ggWyAtMTAyMSAtNDYzIDE3OTQgMTIzMyBdIC9Bc2NlbnQgOTI5IC9EZXNjZW50IC0yMzYgL0NhcEhlaWdodCAwCi9YSGVpZ2h0IDAgL0l0YWxpY0FuZ2xlIDAgL1N0ZW1WIDAgL01heFdpZHRoIDEzNDIgPj4KZW5kb2JqCjE1IDAgb2JqClsgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAKNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCAzMTggNDAxIDQ2MCA4MzggNjM2Cjk1MCA3ODAgMjc1IDM5MCAzOTAgNTAwIDgzOCAzMTggMzYxIDMxOCAzMzcgNjM2IDYzNiA2MzYgNjM2IDYzNiA2MzYgNjM2IDYzNgo2MzYgNjM2IDMzNyAzMzcgODM4IDgzOCA4MzggNTMxIDEwMDAgNjg0IDY4NiA2OTggNzcwIDYzMiA1NzUgNzc1IDc1MiAyOTUKMjk1IDY1NiA1NTcgODYzIDc0OCA3ODcgNjAzIDc4NyA2OTUgNjM1IDYxMSA3MzIgNjg0IDk4OSA2ODUgNjExIDY4NSAzOTAgMzM3CjM5MCA4MzggNTAwIDUwMCA2MTMgNjM1IDU1MCA2MzUgNjE1IDM1MiA2MzUgNjM0IDI3OCAyNzggNTc5IDI3OCA5NzQgNjM0IDYxMgo2MzUgNjM1IDQxMSA1MjEgMzkyIDYzNCA1OTIgODE4IDU5MiA1OTIgNTI1IDYzNiAzMzcgNjM2IDgzOCA2MDAgNjM2IDYwMCAzMTgKMzUyIDUxOCAxMDAwIDUwMCA1MDAgNTAwIDEzNDIgNjM1IDQwMCAxMDcwIDYwMCA2ODUgNjAwIDYwMCAzMTggMzE4IDUxOCA1MTgKNTkwIDUwMCAxMDAwIDUwMCAxMDAwIDUyMSA0MDAgMTAyMyA2MDAgNTI1IDYxMSAzMTggNDAxIDYzNiA2MzYgNjM2IDYzNiAzMzcKNTAwIDUwMCAxMDAwIDQ3MSA2MTIgODM4IDM2MSAxMDAwIDUwMCA1MDAgODM4IDQwMSA0MDEgNTAwIDYzNiA2MzYgMzE4IDUwMAo0MDEgNDcxIDYxMiA5NjkgOTY5IDk2OSA1MzEgNjg0IDY4NCA2ODQgNjg0IDY4NCA2ODQgOTc0IDY5OCA2MzIgNjMyIDYzMiA2MzIKMjk1IDI5NSAyOTUgMjk1IDc3NSA3NDggNzg3IDc4NyA3ODcgNzg3IDc4NyA4MzggNzg3IDczMiA3MzIgNzMyIDczMiA2MTEgNjA1CjYzMCA2MTMgNjEzIDYxMyA2MTMgNjEzIDYxMyA5ODIgNTUwIDYxNSA2MTUgNjE1IDYxNSAyNzggMjc4IDI3OCAyNzggNjEyIDYzNAo2MTIgNjEyIDYxMiA2MTIgNjEyIDgzOCA2MTIgNjM0IDYzNCA2MzQgNjM0IDU5MiA2MzUgNTkyIF0KZW5kb2JqCjE4IDAgb2JqCjw8IC9IIDE5IDAgUiAvUCAyMCAwIFIgL2EgMjEgMCBSIC9jIDIyIDAgUiAvZCAyMyAwIFIgL2UgMjQgMCBSCi9laWdodCAyNSAwIFIgL2ZpdmUgMjYgMCBSIC9mb3VyIDI3IDAgUiAvZyAyOCAwIFIgL2ggMjkgMCBSIC9pIDMwIDAgUgovbCAzMSAwIFIgL20gMzIgMCBSIC9uIDM0IDAgUiAvbmluZSAzNSAwIFIgL28gMzYgMCBSIC9vbmUgMzcgMCBSCi9wZXJpb2QgMzggMCBSIC9xIDM5IDAgUiAvciA0MCAwIFIgL3MgNDEgMCBSIC9zZXZlbiA0MiAwIFIgL3NpeCA0MyAwIFIKL3NwYWNlIDQ0IDAgUiAvdCA0NSAwIFIgL3RocmVlIDQ2IDAgUiAvdHdvIDQ3IDAgUiAvdSA0OCAwIFIgL3YgNDkgMCBSCi96ZXJvIDUwIDAgUiA+PgplbmRvYmoKMyAwIG9iago8PCAvRjEgMTcgMCBSID4+CmVuZG9iago0IDAgb2JqCjw8IC9BMSA8PCAvVHlwZSAvRXh0R1N0YXRlIC9DQSAwIC9jYSAxID4+Ci9BMiA8PCAvVHlwZSAvRXh0R1N0YXRlIC9DQSAxIC9jYSAxID4+ID4+CmVuZG9iago1IDAgb2JqCjw8ID4+CmVuZG9iago2IDAgb2JqCjw8ID4+CmVuZG9iago3IDAgb2JqCjw8IC9JMSAxMyAwIFIgL0kyIDE0IDAgUiAvRjEtRGVqYVZ1U2Fucy1taW51cyAzMyAwIFIgPj4KZW5kb2JqCjEzIDAgb2JqCjw8IC9UeXBlIC9YT2JqZWN0IC9TdWJ0eXBlIC9JbWFnZSAvV2lkdGggNDYzIC9IZWlnaHQgMjMxCi9Db2xvclNwYWNlIFsvSW5kZXhlZCAvRGV2aWNlUkdCIDI1NSAo/v7+/v79/f39/vz7/vv5+/v7/vr3+vr6+fn5/vj1/vfy/vXw+Pj49/f39fX1/vTu/vPs/vHq9PT08/Pz8vLy8fHx/vDn/u7l/u3j7+/v7u7u7e3t7Ozs/evh/erf/enc/efa/ebY/eTW/ePU6urq6enp6Ojo5+fn5ubm5OTk4+Pj/eLR/eDP/d/N4uLi4eHh4ODg3t7e/d3L/dzJ/dvH/NjE/NbB+9S++9K8+9C5+s62+sy03d3d29vb2tra2NjY19fX1dXV1NTU0tLS0dHRz8/Pzs7OzMzMy8vL+sqx+ceu+cWr+cOp+MGmycnJ+L+j+L2h97ue97mb97aY9rSW9rKT9bCQ9a6OyMjIxsbGxcXFw8PDwsLCwMDAv7+/vb29vLy8urq6ubm5t7e3tbW1s7OzsbGxr6+vra2t9ayL9KqI9KiG9KaD86OA8qB+8Z5875t67ph47ZZ27JN065By6o1w6Ituq6urqampp6enpaWl54hs5oVqo6OjoaGhn5+fnZ2dm5ubmZmZl5eXlZWVk5OTkZGRj4+PjY2Ni4uLiYmJh4eHhISE5YNo5IBl4n1j4Xth4Hhf33Vd3nJb3XBZ221X2mpV2WhT2GVR12JP1mBN1F1LgoKCgICAfX19e3t7eXl5d3d3dHR0cnJycHBwbW1ta2traWlpZ2dnZGRkYmJiYGBgXl5e01pK0VdJ0FRHzlFGzU9EzExDyklCyUZBx0M/xkA+W1tbWVlZV1dXVFRUUlJSUFBQTk5OxT48wzs7wjg6wDU4vzI3vjA2vC00uyozuScyuCQxtiEvtR8utBwtshkrsBcqrRYqqhVcKacUXCmkE1wooRJcKJ4RJ5sQJ5kQJ5YPJpMOJpBcciWNDCWKCySHXG4khAkjgQgjfgciewYieAUhdQQhcgMgbwIgbAEfaQAfZwAfTExMSUlJSEhIRUVFREREQUFBQEBAPT09PDw8OTk5ODg4NTU1NDQ0MTExMDAwLi4uLCwsKioqXChcKFwoJiYmJCQkIiIiICAgHh4eHBwcGhoaKV0KL0JpdHNQZXJDb21wb25lbnQgOCAvRmlsdGVyIC9GbGF0ZURlY29kZQovRGVjb2RlUGFybXMgPDwgL1ByZWRpY3RvciAxMCAvQ29sb3JzIDEgL0NvbHVtbnMgNDYzID4+IC9MZW5ndGggNTEgMCBSID4+CnN0cmVhbQp4nO3dd5RsWVXH8WfAQWEwYQBFAR0GEIURs6OCShAVAwYMoKPigAqYMWNGBkEwYADBiJgwYAIEFcUso5hFgvBAUUDECTryZry/T68+p+/tW9Xd9/XrGmrt3x+z5nVXV52zf7tWf3vvfU6depPoP6IrBt0m+sPopdHHRj8RXTvo86Pvi14RfW/0BVG++ePRx0X/EnmOCwflWa/w/G8WvSD6x+iWg54RXRN9VPT06HT04EHfEV0XfWv0pVG+9yvR3aP/i34zeq/ohYP+MrpB9Orov6N3HfTHkT19TPSTUZZ97SWDfiCyp8dGXxh54ScP+sTInv4gul105aBXRW8R/XX0D9G7Rc8a9L/RXaNfj+zpi6NHDPLU3xx9eeSFf2nQvSN7+vnovaN/ji4fdMPotdHro5tHfxK9ZNCHRD8dnSo/y8/ys/wsP8vP8vPofnryb48eOsgL/2p0t0iofyu69aB/iv4qOhUJGLsuGPRH0aok+P6oJ8EDonzzx6KPjwTseVGS4MIE7N+jg5Pg16KWBN8Z9ST4siiv+8vRPSIB+4VoJgleE7UkkAWJ10skwU9Fq5Lg0ujAJLjywCS4S/Qb0aok+IqoJYEseEMkCcrP8rP8LD/Lz/Kz/FzgJ759UnRm0H9FnxfZwV9EfjSreo/o4dHVkZf/tigL+r3IQ0XnE6L7D/rPSBBYDNN+JHrZIBkC8j4s+u3IwmPZHaOvjhLzF8qVxw96+wjMeVoWfEZ0n0HCJBJi8rZRwsH/q6Kvjd43koJZ8Z9FHxhxRY4K0aMGvWXUyftvowcNkhxyzop/NnqXKBkrCV4UPSaSo3IoD/3X6COjZOCD/z56+SD2jP04A2ZX+NEN6X6Un+Vn+Vl+lp8n5KfF5Lfxsy4a9JWR8Pll/kPRTaKGAkoUnx19UgSP8gK/E71zpK7T4veN0R0iJSOL+fMoJQ3VJUsWvidGbx6lBJQtvvzvIlDw0VFwwor98n/PCI28eFf+5es/F3lsfshPex7P6LkVmvJqXtcKoBBcU3LJOq3Y2u3CfrIxO7TXd4rs32MDb2LzWZF4iRycSixFVXxFWswTfC6M/dgxJFnv8T8c8aMXuMZ+lJ/lZ/lZfpaf5Wf5ucBPPPXA6JWDMJWVCpgV3SJKl+xvIkvWxVFGsu2LB0EyZGhFr4sCljpwXup3I+AHFNME+voIQHLWNiwhNRx1qc+JUOr3RLvZ8wrsCGGVo94nCo1KDivwf/nSo24/SCfP49nuOeRi8PNtos+N1J7e0JT1KHC9VWS1WfblWb+d2JNikH1+V5SdW4Fo/GjEmg+NwsmiZwVoXMksMeYKCuaAQMSUVwa6+aQ05N0ghJD/9wd9agSuy8/ys/wsP8vP8rP8XODneO+3H+9d1NreRVRs7V28bTsvzAuu8Kft/fR475xte3/deO9ywt6TJU9fsfdbtL3LufHeL12x97b1PXvXJQweWoFioCkoLasPiFKAU5O0AmSciuPjbzXItM9TI4U2rPlvUeqJypppQD70TyM43eaqvihStdT/0nfMClTtUDAaV5N8QpSumhXor6mWGi7S00zh838iuyg/y8/ys/wsP8vP8nOBn/Z+z+iyQRYkQr5rpRAr0TfDrByXTuN9vBTcS6y8Hna0A9M19xp04wgnIz2T1UZ42g5kgtgaQH7rKA1GIz9Gh9IqfVkDa21Ldb0Pim4aSbB4oRsqyWzAzE4GrrVtw+EXS4eHRCaP0s4Ubs/PPLZ/Q3TnQQaJlAD1XKVgsoeB6oMqi18TmRPKqJWerhKg7ubpphgisaWbBqYfyk9roXprXdvk5Z4yyJC3QiafvV8stKWbXZSf5Wf5WX6Wn+Vn+bnAz+5bppQeGZl0Msj8yRGeev6gl459M0UF98LD50WfGekmGpTatW3HN9NeGoZmoW406NMjY1Bqbwiw+eZI2OOiD44AYap/hrDnfAtvI0bDZbqSXxKFk9UqJ76ZqXr0oPeLIC9Ah5jm2OKb5+cbnP66KNXFMO6tzGs/Jxr7BkSNyGlgmtw2FPawQaB44lvm8H4mAugZUL8AwpqHe+4g4+Seu/eF/Q2SY3fvHpmfKz/Lz/Kz/Cw/T8hPPAQUspXvjjSN/CJu4HJdXpMrjPLKDvwYZM5vaeEzRZ0IPRprJAmMyPRAQhu7DTWxEhnou6ksKVEFbzSftI/M4Mi563alIsJUNOIYUsa3bxtBqsDYvb4qypiQ5BOTDldWFWNVkZSjYIb6mDpY6lgqUXANr6jm5MXZYC34kf/OiocrjaQLMH4Ebrx/2iCJyv+ewDGqncW+nFHKceppITgHstWkpCw2c0Q8Za2+lvKz/Cw/y8/ys/wsPxf46b/aTelYqQVpGjmGpW/zdlEKGWjUUWvkpnSDdLMYq0LATmBhx8wNI0wlDQT8ERGGS7VIaQjp9cyxlixSrQprKu6oYd1skPPMyFuRCF0605bm3um2FrEStdSTFLNceeMcGXrXmct0kCkj7T9Z3OtjLYuVtWSxA3XBeAz6jpED5upJsjhnwOayWDYlixW+cK+RIrNJqRGZlcrM1SOQtGwdZ7F/+bpHeGyyWInJ85Sf5Wf5WX6Wn+Vn+bnAT3xroCfdMH0hNTkHmJ4Z+fkEuJfXeuBxZopVDpEjQIysLJYumaA5EoWAkV66YQ/KIHNo72m6PrAO97aJ68Z9p1nTbstx1sswt2KgM+tOo6foJWkgMhY0KB5OTsvpKSqOQJRJ4/34l697xLOj/JASaMp0D/OMnjsvIq28rpS1FifWsjzlSyuW4qdH+/El7tq1/Qf0Hc0XGwVO8RK5Ow1SShVV2admKOYJPhf4UX6Wn+Vn+Vl+lp/l5wI/G8ddG6rT51PSU7rKVXRP/pYoBThzxupOUCrDNXc2SZSimqPgJmZUz/Tnwr4d8jrpKWylIelCXsUtfVRTN2qG6RdiUBMzMNs5r5Td3GOkJqn2pvXZ8bhtBwti53CyqqDO7DdF94u0SdNVVdszQP1pEVT9xSiNxxQtn98zbbyffmoe7RttSt3Qrc/mqR1/0yt90yjlSP1ljA9QIXxAX1nzqhXvHJbBb3+D8DOVxcsyeaTquqefXX6Wn+Vn+Vl+lp/l55H8xLcpo90sFGVk2nAylPJ65q8CrvBQ5UuYsKBAtu5bj6YVJaIOjlkpGpMmdhBK08rD1FakGIi1M4Sti2mSSlnMubAYjiSFW6kP7gp66oxiOxN48iV0KaMUA1Mo1JA05awhq6SpcxrWZIiCpeKea48zOqZnrAip2SlNsHDSTTkOIvePmeDIbrjm4uUBHupPCr7p/+YZPbXCp8TG2hIy496oXlaWn+Vn+Vl+lp/lZ/m5wM8xRWn9IUYoZeAISqb154CYySjlL4zoIxbSfTO+7pML8hkMt1TXS48S5KlkIWAdQs3RULAz9Cpf+qjgt9fGol7vc/Y/mIkkzXcFP5+r8veDUbBU9c+FQO8fKcqFk98hckuRQ+pajY6EpbYJ4xGjhiy+d1N3u3fxTJuUH4XrDIgWL0VCLdv0UY1rKXyamgOzDtnlBJ1B+fMjfyfkPNqtMw73KZGpMY1dLVsfl5WPvXA1tSl/ZUQp20fbmsrP8rP8LD/LzxPyEw8ZEco8TJjoTjwCKPkcoEuYlxqRAWd3yliu3/kCkF34FS4Ittd++Z9pZ5nH4YABvgmpPN5Peo48meB4flDgNRN4ZCBMDEEjukmpImnPQSoJ5oOcwlWSzDkyLS7jR643zjC3yWUVKg08MZQr4SSBlEP4STSTfLgKYY0zcIJfclH0MVgSkitaZY6yy+6QnW6brHeln6NlDpKHBA15q2E51I3NVNCSF86Q76kPlZ/lZ/lZfpaf5Wf5eSQ/FwWYlQLM3gRYV20S4OSFAJs97gF2f/A4wPpuBnEMGifAmFXBRoCN2STAakE9wCpC8DiRWxVgX+oB9ng/mQDD7JkAPyeDzz6NQU1qHGB1IwEGqD3AmfjpAXa3wH0jZbfMM6mKKb35i0G1LO0801qqaIpK/sJQXkshDmarus29ORpwl5/lZ/lZfpaf5Wf5ucBPfOuSvVTEBF5Z7MMjgztaM6ntMcqEsg99gpiWlbqWrpqlKZxB0qxP10fdEAFbpHZbUkXLTc1MDlluA+KZJbe820k8j7dR2ZfqnP17fgU4gUka6l2pXMpFR8WMjCcfzeE4si8rHa1zX17wXplOX4w12mQpcLJMjrq8mJVafDkdj0Flq4sEwKmUzVXGSqDSVh3UrFGSRg9Q+dJIFcLWDcyUFcx2jA4eS2WV0lDzixtwl5/lZ/lZfpaf5Wf5ucDPtbESZfFO4OdixanECpxxkZ9ileqc01A9VsaStAOTHO72ESs1RvMwDnBlitpFxWIl53J/443y+aJw0jC3Y1hipX6XaR/4KVauQVI4CyeLlYKaWLkwIG3F541jhXTNgotVAFjfVcEOGbce7UyBbw6nPV6oBT3RVzJkCGucRQuzg2sGYnmE7VakFCHVAU02qU5ibfcHpGipc2oQvfwsP8vP8rP8LD/LzwV+ipWopQdqsNqZLhcbqliJcuaY0K7oq+bl4pv7Y94MROuS4l7lLNf0hH2xo4NdCJjPDr/Hdazp4h9lRBCqqpiJLPliErtDcTLqykbGqBbfNjw+07SqLLaKmOdSOU/d0vnqqxpAZ+ZZGVRiA3dz5tYeoLcdOC3F3UOAqfMHgPudlPoku7ZyxtIkvLlsdVNZb3g6w2swW93QzDvW9jdIOtbuH/K3SflZfpaf5Wf5eUJ+HmF717TttXLMVYK7anthJf0fJ5kQk/NIDkvHTy03562NNBtANh4UflJ/cgJHwUZhKncu6zsZ5jbj5LQznMppJu05SGWIyIc2hKuMLztLpJBjkNkBnlRzHJ++Q6SkYxZcFy8X80Eth6F8ziPgyrEoB5ZAl+lnqKLFl4/DMIek1KPX59w6BksrziluR9MdZdf/CxOBMbcf9uoPdgpEdSwDV53NWoOQys/ys/wsP8vP8rP8XOAnvrXR9MwMVjuL5V4apOs4VAAK7eoXOUh918a8iaiuGu5V9HGAK+0mnyCl+YSAWYPhGgW7LlDfjZVMDfyxGAuCYt4HEcEiMpYcKFKuJGm05+SQypW0apws3TosOwrdiFl6dmyWtw2dJ+28xs9dhyPple+c8btnTNiT91Fnbeve5e2dd1T5WX6Wn+Vn+Vl+lp8L/Dzq600qYuOy2MzrXTF+PdFsGKyk53j7qxoLc6TxsGIg3zjY6mbc5TPHed8qaPJChsiVcLIipDySUXJLliXdHKOTgXJRVsrPVNZkrNxVXpPPSWw1NhAt6+W/Fl/eEgayvEu8X9xDgKkzSe6iYjcBOPVm1iif9akHaEJbI9AtPaa20wjzwVf6Yj5mA2u7XCDDSC4+1CUsP8vP8rP8LD/Lz/JzgZ9ez1W7uQPOOnymgEnhTrqpm1m3HWiOqqph3tTY7NOOXXusP5dI6NbdoxGwEp1eXiKnhap+p4+q2ec6v7T+FPh0Ag0d6a2m+qdNqBjoLqQxHmu3QmQ9V8ff4rXmq8pih2Vt2EbMipAdmyVSQ+fenm0ArdcpDSVkI+lXJ1EnOC2VX7+rxtV7wHoM152wV2F21xi4y8/ys/wsP8vP8rP8XODnWr5dR7sT5p28yoh9r+lL68ttFDwuXe2wcNv7BIoTptc0MhbDjscJcEdk0edDDFEVnMAyBxsxc7djM++TBNBZXsgQuZKkmUC03JJlSTdtWxkoF2Wl/Eyiyli5K4vlcxIbXMt1WS//vRMaZnuXeL901s6bybvK+6v8LD/Lz/Kz/DwhPzsPpZpxXoMiAzfISA8thQ90ZOLaWI4Dyhgpg7y6ajhJ4aTBkhsBLxoTk3ZbVqXsYvbHcl01DKAytG0/Johtr/XijA4hKSWdMU5pzwmY6s+lu2AlokpDHa6UixJ4xaM5zMoYE8vMPD+yAVeKUB26jEY38oJeilYdv5xbbwwmfZ7a1GhMkpm/HhPZM8ZYZhjr2U2tQQjQys/ys/wsP8vP8rP8XODnEfj2sMw77q2tIuARBe9h4QkQ74fiq2fIuONxQ+RVnNxheUzME2xu7NzQeUcNoHtEBbiR9DObBR2nmdSYmoHdz0bXDGd9J2yJMcZs6dNZO7l1WQPu8rP8LD/Lz/Kz/Cw/F/iJb90yvMu4O5DrCH4n3THtaqbddMS8N28aw69T4JpsFzY1DEbBrgu8Y2PhBsR42GH/MRTfpZGx8tcEjxsiq5TdcxeU79vUYbm17e7XNMHmS3bV23kdoPdD9I4ymjTB6TFTP7xpAtaREfNO2GPMnmPtXd7eAe7ys/wsP8vP8rP8LD8X+HmWfLsIftdR8FoenoPiOTw+kJNniHmCzRN2HvHzKojeg9JjnB4z9WubJmDdNCbs3uGdY+3W8KXys/wsP8vP8rP8LD8X+Dnm29YIvcGEdBvtTpqjnXnH3NvhtzVNJwTcKHjcR72gA3GD4ts23W5XnYw7Hu9H5B1Gbpw8geUZYp5gc0PnMT/frfFzh+hG0vdehdOdqXe5ekczcN0JewazJ6w94u0HlJ/lZ/lZfpaf5Wf5ucDPc8m3x0TBy/D4YFA+JDavQOdVEL0ep/cj9Uq4niHsGcyeAHf5WX6Wn+Vn+XlCfnYeGjHRHijqZDSiowkjnT/WflbaQ0wTbBqh0w47Nc1A1BxJNc0g1VyzbhVhde1HrQlwXTTWxTPo1fBrwmAjDtsDYg3GumawbIbNusrP8rP8LD/Lz/Kz/Fzg56bp9qy1HI+XE/PZkfRRwPoImE3lZ/l5/VL5WX6Wn9dflZ8jP8d8O4HcVbh73n7dcEY3Hun8/brJjFbx8Aow3kPGKxB5wskzsDwh5oPZeQVET1F6BqdHSL1HK+B6piM4x9oduMvP8rP8LD/Lz/Kz/Fzg56bx9HqtY2XnEyHs8nOdys/tUvm5XSo/t0tvhH7O8O160p1pjq7j3vUEvJ+C1/LwejKeabeu4+SVxLyWnVcB9EqUXsfU6+l6P2GvZW0qP8vP8rP8LD9PyM9NI0dprY6KU+Xn9Vvl53ap/NwulZ/bpSP7uZZvDybdw3HvUTD4QB4+JB4fEpQPJOZDAvQRcPoQYH04zJ7h7fKz/Cw/y8/ys/wsPxf4uWmAKx2rys/tUvm5XSo/t0vl53bpsHy7GHmXEfBiKF7GyIuweTFJnwVdH8ja5Wf5WX6Wn+Vn+Vl+LvBz00BWOlaVn9ul8nO7VH5ul86Ch46Vk84RRB0rXJ0j8jpWIis/y8/ys/wsP8vP8nOBn5sGstKxqvzcLpWf26Xyc7tUfm6XzgHfbgyDNwXLG2PqGZWf5Wf5WX6Wn+Vn+bnAz00DWelYVX5ul8rP7VL5uV0qP7dLm+PbxdoUGC/TCUN0+XmOVX4epE07dDSVnwdp0w4dTeXnQdq0Q0fTSfu5aSArHavKz+1S+bldKj+3S2+EPHT82jQzHaPKz1Pl57Zp0yYco8rPU+XntmnTJhyjim+3S+Xndqn83C6Vn9ul8nO79P8xNVtzCmVuZHN0cmVhbQplbmRvYmoKNTEgMCBvYmoKNTYwOAplbmRvYmoKMTQgMCBvYmoKPDwgL1R5cGUgL1hPYmplY3QgL1N1YnR5cGUgL0ltYWdlIC9XaWR0aCAxMiAvSGVpZ2h0IDIzMQovQ29sb3JTcGFjZSBbL0luZGV4ZWQgL0RldmljZVJHQiAyMzAgKP/+/v7+/v/9/Pz8/P/7+vv7+//69//59fr6+v/38/728fj4+P707/f39/b29vX19f7y6v7w6P7v5vPz8/Ly8v7t5P7s4f7r3/Dw8O/v7+3t7ezs7Ovr6+rq6v7p3f7o2/7m2f7l1v3i0v3h0P3fzunp6efn5+bm5uTk5OLi4uHh4eDg4N/f3/3ey/3cyf3bx/3ZxPzXwvzVv93d3dzc3Nra2tnZ2dfX19TU1NPT09HR0fvQufvOt/vMtPrKsfrIr/nGrPnEqdDQ0M7OzvnCp/i/pPi7nve5nPe3mfe1lvazlM3NzcvLy8rKysjIyMXFxcTExMLCwsHBwb+/v76+vry8vLu7u7m5ubW1tbOzs/axkfavjvWsi/WqifSmg/OkgfKhf/GeffCce++Zee6Wd+yTdLGxsa+vr62traurq6mpqaenp6WlpaGhoZ+fn52dnZubm5mZmZeXl5WVlZOTk+uRcumLbuiJbOaGauWDaOSAZuN+ZOJ7YuF4YN92Xt5zXFzcbleRkZGPj4+Li4uJiYnba1XaaFPYZVHXY0/WYE3VXUzTWkrSWEnPUkbOT0XMTETLSULJR0HIRECHh4eFhYWCgoKAgIB+fn58fHx5eXl1dXVzc3NwcHBubm5sbGxpaWlnZ2dlZWVjY2NeXl5cXFxcXFxaWlpXV1dVVVVTU1NQUFBOTk5MTExISEhGRkZERETGQT7FPj3EOzzBNjm/Mzi+MDa9LTW7KjS6XCgyuCUxtyIwth8usxkssRgrrhcqqxYqqBVcKaUUXCmiE1wonxJcKJwRJ5kQJ5MOJpBcciaNDCWKCyWHXG4khAkkgQgjQkJCQEBAPj4+PDw8Ojo6ODg4NjY2MjIyMDAwLi4uLCwsKioqXChcKFwoJiYmfwgjfAcidgUhcwQhcAMgbQIgagEfJCQkIiIiZwAfHh4eHBwcGhoaKV0KL0JpdHNQZXJDb21wb25lbnQgOCAvRmlsdGVyIC9GbGF0ZURlY29kZQovRGVjb2RlUGFybXMgPDwgL1ByZWRpY3RvciAxMCAvQ29sb3JzIDEgL0NvbHVtbnMgMTIgPj4gL0xlbmd0aCA1MiAwIFIgPj4Kc3RyZWFtCnicVcHVYggAAEDRq6ZnpntmprtjYmwY0zbd3d2xidl0D9Pd3d3d3TV8h9frHP4Kf4Tfwi/hp/BR+CC8F94Jb4U3wmvhlfBSeCE8F54JT4UnwjHhqHBEOCwcEg4KB4T9wj5hr7BH2C3sEnYKO4TtwjZhq7BFSBM2C5uEjUKqsEFYL6wT1goLhQXCfCFRmCxMEiYKE4TxwjhhrDBGGC2MEkYKI4ThwjBhqNBD6C7EC3FCN6Gr0EXoLHQSOgodhPZCrNBOaCu0ECKFhkIDob5QT6gr1BFqC7WEykIloaJQQSgvlBPChbJCCaG4UEwoKhQRCgv5hXxCHiFQyC3kFLIJWYRMQgbhPxmFzEKAkFXILuQQcglBQl4hWCggFBQKCSWFUkKIUFoIFcoIYUIVoapQTagu1BBqCo2ExkKE0ERoKjQTmgsthSghWmgltBbaCDFCT6GX0FvoI/QV+gn9hQHCQGGQMFgYIkwRpgrThOnCDGGmMEuYLcwR5grzhARhkZAkLBaShRRhibBUWCYsF1YIK4VVwmphjXBcOCGcFE4Jp4UzwlnhnHBeuCBcFC4Jl4UrwlXhmnBduCHcFG4Jt4U7wl3hnnBfeCA8FB4Jj4VPwmfhi/BV+CZ8F34I6fIPtR/deQplbmRzdHJlYW0KZW5kb2JqCjUyIDAgb2JqCjQ4MAplbmRvYmoKMiAwIG9iago8PCAvVHlwZSAvUGFnZXMgL0tpZHMgWyAxMSAwIFIgXSAvQ291bnQgMSA+PgplbmRvYmoKNTMgMCBvYmoKPDwgL0NyZWF0b3IgKE1hdHBsb3RsaWIgdjMuNy4xLCBodHRwczovL21hdHBsb3RsaWIub3JnKQovUHJvZHVjZXIgKE1hdHBsb3RsaWIgcGRmIGJhY2tlbmQgdjMuNy4xKSAvQ3JlYXRpb25EYXRlIChEOjIwMjQwMjI2MTgxMjM0WikKPj4KZW5kb2JqCnhyZWYKMCA1NAowMDAwMDAwMDAwIDY1NTM1IGYgCjAwMDAwMDAwMTYgMDAwMDAgbiAKMDAwMDAyMDQxNSAwMDAwMCBuIAowMDAwMDEyMDY2IDAwMDAwIG4gCjAwMDAwMTIwOTggMDAwMDAgbiAKMDAwMDAxMjE5NyAwMDAwMCBuIAowMDAwMDEyMjE4IDAwMDAwIG4gCjAwMDAwMTIyMzkgMDAwMDAgbiAKMDAwMDAwMDA2NSAwMDAwMCBuIAowMDAwMDAwMzQyIDAwMDAwIG4gCjAwMDAwMDE0NTkgMDAwMDAgbiAKMDAwMDAwMDIwOCAwMDAwMCBuIAowMDAwMDAxNDM4IDAwMDAwIG4gCjAwMDAwMTIzMTAgMDAwMDAgbiAKMDAwMDAxODk2MyAwMDAwMCBuIAowMDAwMDEwNjQyIDAwMDAwIG4gCjAwMDAwMTA0MzUgMDAwMDAgbiAKMDAwMDAwOTk2MyAwMDAwMCBuIAowMDAwMDExNjk1IDAwMDAwIG4gCjAwMDAwMDE0NzkgMDAwMDAgbiAKMDAwMDAwMTYzMCAwMDAwMCBuIAowMDAwMDAxODczIDAwMDAwIG4gCjAwMDAwMDIyNTMgMDAwMDAgbiAKMDAwMDAwMjU1OCAwMDAwMCBuIAowMDAwMDAyODYyIDAwMDAwIG4gCjAwMDAwMDMxODQgMDAwMDAgbiAKMDAwMDAwMzY1MiAwMDAwMCBuIAowMDAwMDAzOTc0IDAwMDAwIG4gCjAwMDAwMDQxNDAgMDAwMDAgbiAKMDAwMDAwNDU1NCAwMDAwMCBuIAowMDAwMDA0NzkxIDAwMDAwIG4gCjAwMDAwMDQ5MzUgMDAwMDAgbiAKMDAwMDAwNTA1NCAwMDAwMCBuIAowMDAwMDA1Mzg1IDAwMDAwIG4gCjAwMDAwMDU1NTcgMDAwMDAgbiAKMDAwMDAwNTc5MyAwMDAwMCBuIAowMDAwMDA2MTg4IDAwMDAwIG4gCjAwMDAwMDY0NzkgMDAwMDAgbiAKMDAwMDAwNjYzNCAwMDAwMCBuIAowMDAwMDA2NzU3IDAwMDAwIG4gCjAwMDAwMDcwNzMgMDAwMDAgbiAKMDAwMDAwNzMwNiAwMDAwMCBuIAowMDAwMDA3NzEzIDAwMDAwIG4gCjAwMDAwMDc4NTUgMDAwMDAgbiAKMDAwMDAwODI0OCAwMDAwMCBuIAowMDAwMDA4MzM4IDAwMDAwIG4gCjAwMDAwMDg1NDQgMDAwMDAgbiAKMDAwMDAwODk1NyAwMDAwMCBuIAowMDAwMDA5MjgxIDAwMDAwIG4gCjAwMDAwMDk1MjggMDAwMDAgbiAKMDAwMDAwOTY3NSAwMDAwMCBuIAowMDAwMDE4OTQyIDAwMDAwIG4gCjAwMDAwMjAzOTUgMDAwMDAgbiAKMDAwMDAyMDQ3NSAwMDAwMCBuIAp0cmFpbGVyCjw8IC9TaXplIDU0IC9Sb290IDEgMCBSIC9JbmZvIDUzIDAgUiA+PgpzdGFydHhyZWYKMjA2MjYKJSVFT0YK\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "encod_block = PositionalEncoding(d_model=48, max_len=96)\n",
        "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n",
        "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n",
        "fig.colorbar(pos, ax=ax)\n",
        "ax.set_xlabel(\"Position in sequence\")\n",
        "ax.set_ylabel(\"Hidden dimension\")\n",
        "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
        "ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n",
        "ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMw8HK3RnTTF"
      },
      "source": [
        "You can clearly see the sine and cosine waves with different wavelengths that encode the position in the hidden dimensions. Specifically, we can look at the sine/cosine wave for each hidden dimension separately, to get a better intuition of the pattern. Below we visualize the positional encoding for the hidden dimensions $1$, $2$, $3$ and $4$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "VVTNjjJRnTTF",
        "outputId": "c5bc3fe7-e00f-4a66-bc1d-2d0a3e9fce9b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 4 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"721.920312pt\" height=\"284.134375pt\" viewBox=\"0 0 721.920312 284.134375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-02-26T18:12:36.465950</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 284.134375 \nL 721.920312 284.134375 \nL 721.920312 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.120313 101.518125 \nL 349.483949 101.518125 \nL 349.483949 22.318125 \nL 45.120313 22.318125 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 58.955023 101.518125 \nL 58.955023 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 1 -->\n      <g style=\"fill: #262626\" transform=\"translate(55.773773 118.616563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path d=\"M 77.401304 101.518125 \nL 77.401304 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2 -->\n      <g style=\"fill: #262626\" transform=\"translate(74.220054 118.616563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path d=\"M 95.847585 101.518125 \nL 95.847585 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 3 -->\n      <g style=\"fill: #262626\" transform=\"translate(92.666335 118.616563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path d=\"M 114.293866 101.518125 \nL 114.293866 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 4 -->\n      <g style=\"fill: #262626\" transform=\"translate(111.112616 118.616563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path d=\"M 132.740147 101.518125 \nL 132.740147 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 5 -->\n      <g style=\"fill: #262626\" transform=\"translate(129.558897 118.616563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path d=\"M 151.186428 101.518125 \nL 151.186428 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 6 -->\n      <g style=\"fill: #262626\" transform=\"translate(148.005178 118.616563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <path d=\"M 169.632709 101.518125 \nL 169.632709 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 7 -->\n      <g style=\"fill: #262626\" transform=\"translate(166.451459 118.616563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <path d=\"M 188.07899 101.518125 \nL 188.07899 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 8 -->\n      <g style=\"fill: #262626\" transform=\"translate(184.89774 118.616563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <path d=\"M 206.525271 101.518125 \nL 206.525271 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 9 -->\n      <g style=\"fill: #262626\" transform=\"translate(203.344021 118.616563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-39\" d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <path d=\"M 224.971552 101.518125 \nL 224.971552 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 10 -->\n      <g style=\"fill: #262626\" transform=\"translate(218.609052 118.616563) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <path d=\"M 243.417833 101.518125 \nL 243.417833 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 11 -->\n      <g style=\"fill: #262626\" transform=\"translate(237.055333 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <path d=\"M 261.864114 101.518125 \nL 261.864114 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 12 -->\n      <g style=\"fill: #262626\" transform=\"translate(255.501614 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <path d=\"M 280.310395 101.518125 \nL 280.310395 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 13 -->\n      <g style=\"fill: #262626\" transform=\"translate(273.947895 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <path d=\"M 298.756676 101.518125 \nL 298.756676 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_14\">\n      <!-- 14 -->\n      <g style=\"fill: #262626\" transform=\"translate(292.394176 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <path d=\"M 317.202957 101.518125 \nL 317.202957 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_15\">\n      <!-- 15 -->\n      <g style=\"fill: #262626\" transform=\"translate(310.840457 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <path d=\"M 335.649238 101.518125 \nL 335.649238 22.318125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_16\">\n      <!-- 16 -->\n      <g style=\"fill: #262626\" transform=\"translate(329.286738 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- Position in sequence -->\n     <g style=\"fill: #262626\" transform=\"translate(146.006818 132.294688) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \nL 1259 2394 \nL 2053 2394 \nQ 2494 2394 2734 2622 \nQ 2975 2850 2975 3272 \nQ 2975 3691 2734 3919 \nQ 2494 4147 2053 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 2053 4666 \nQ 2838 4666 3239 4311 \nQ 3641 3956 3641 3272 \nQ 3641 2581 3239 2228 \nQ 2838 1875 2053 1875 \nL 1259 1875 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\nM 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 -1331 \nL 2906 -1331 \nL 2906 525 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_17\">\n      <path d=\"M 45.120313 94.918125 \nL 349.483949 94.918125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_18\">\n      <!-- −1 -->\n      <g style=\"fill: #262626\" transform=\"translate(20.878125 98.717344) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_18\">\n      <path d=\"M 45.120313 61.918125 \nL 349.483949 61.918125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_19\">\n      <!-- 0 -->\n      <g style=\"fill: #262626\" transform=\"translate(29.257813 65.717344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path d=\"M 45.120313 28.918125 \nL 349.483949 28.918125 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_20\">\n      <!-- 1 -->\n      <g style=\"fill: #262626\" transform=\"translate(29.257813 32.717344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_21\">\n     <!-- Positional encoding -->\n     <g style=\"fill: #262626\" transform=\"translate(14.798438 110.384531) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 58.955023 61.918125 \nL 77.401304 34.149583 \nL 95.847585 31.911311 \nL 114.293866 57.261165 \nL 132.740147 86.892607 \nL 151.186428 93.562627 \nL 169.632709 71.138836 \nL 188.07899 40.237567 \nL 206.525271 29.269303 \nL 224.971552 48.318215 \nL 243.417833 79.870822 \nL 261.864114 94.917802 \nL 280.310395 79.625032 \nL 298.756676 48.052613 \nL 317.202957 29.228081 \nL 335.649238 40.458625 \n\" clip-path=\"url(#p009c6d3ebf)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: round\"/>\n    <defs>\n     <path id=\"m060c842614\" d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" style=\"stroke: #000000\"/>\n    </defs>\n    <g clip-path=\"url(#p009c6d3ebf)\">\n     <use xlink:href=\"#m060c842614\" x=\"58.955023\" y=\"61.918125\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"77.401304\" y=\"34.149583\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"95.847585\" y=\"31.911311\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"114.293866\" y=\"57.261165\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"132.740147\" y=\"86.892607\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"151.186428\" y=\"93.562627\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"169.632709\" y=\"71.138836\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"188.07899\" y=\"40.237567\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"206.525271\" y=\"29.269303\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"224.971552\" y=\"48.318215\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"243.417833\" y=\"79.870822\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"261.864114\" y=\"94.917802\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"280.310395\" y=\"79.625032\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"298.756676\" y=\"48.052613\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"317.202957\" y=\"29.228081\" style=\"fill: #1f77b4; stroke: #000000\"/>\n     <use xlink:href=\"#m060c842614\" x=\"335.649238\" y=\"40.458625\" style=\"fill: #1f77b4; stroke: #000000\"/>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 45.120313 101.518125 \nL 45.120313 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 349.483949 101.518125 \nL 349.483949 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.120313 101.518125 \nL 349.483949 101.518125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.120313 22.318125 \nL 349.483949 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_22\">\n    <!-- Encoding in hidden dimension 1 -->\n    <g style=\"fill: #262626\" transform=\"translate(101.073381 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-45\" d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-45\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n     <use xlink:href=\"#DejaVuSans-63\" x=\"126.5625\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"181.542969\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"242.724609\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"306.201172\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"333.984375\"/>\n     <use xlink:href=\"#DejaVuSans-67\" x=\"397.363281\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"460.839844\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"492.626953\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"520.410156\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"583.789062\"/>\n     <use xlink:href=\"#DejaVuSans-68\" x=\"615.576172\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"678.955078\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"706.738281\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"770.214844\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"833.691406\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"895.214844\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"958.59375\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"990.380859\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1053.857422\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"1081.640625\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1179.052734\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1240.576172\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"1303.955078\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1356.054688\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"1383.837891\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1445.019531\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"1508.398438\"/>\n     <use xlink:href=\"#DejaVuSans-31\" x=\"1540.185547\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 410.356676 101.518125 \nL 714.720312 101.518125 \nL 714.720312 22.318125 \nL 410.356676 22.318125 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_17\">\n     <g id=\"line2d_21\">\n      <path d=\"M 424.191387 101.518125 \nL 424.191387 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_23\">\n      <!-- 1 -->\n      <g style=\"fill: #262626\" transform=\"translate(421.010137 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_22\">\n      <path d=\"M 442.637668 101.518125 \nL 442.637668 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_24\">\n      <!-- 2 -->\n      <g style=\"fill: #262626\" transform=\"translate(439.456418 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_19\">\n     <g id=\"line2d_23\">\n      <path d=\"M 461.083949 101.518125 \nL 461.083949 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_25\">\n      <!-- 3 -->\n      <g style=\"fill: #262626\" transform=\"translate(457.902699 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_20\">\n     <g id=\"line2d_24\">\n      <path d=\"M 479.53023 101.518125 \nL 479.53023 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_26\">\n      <!-- 4 -->\n      <g style=\"fill: #262626\" transform=\"translate(476.34898 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_21\">\n     <g id=\"line2d_25\">\n      <path d=\"M 497.976511 101.518125 \nL 497.976511 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_27\">\n      <!-- 5 -->\n      <g style=\"fill: #262626\" transform=\"translate(494.795261 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_22\">\n     <g id=\"line2d_26\">\n      <path d=\"M 516.422792 101.518125 \nL 516.422792 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_28\">\n      <!-- 6 -->\n      <g style=\"fill: #262626\" transform=\"translate(513.241542 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_23\">\n     <g id=\"line2d_27\">\n      <path d=\"M 534.869073 101.518125 \nL 534.869073 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_29\">\n      <!-- 7 -->\n      <g style=\"fill: #262626\" transform=\"translate(531.687823 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-37\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_24\">\n     <g id=\"line2d_28\">\n      <path d=\"M 553.315354 101.518125 \nL 553.315354 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_30\">\n      <!-- 8 -->\n      <g style=\"fill: #262626\" transform=\"translate(550.134104 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_25\">\n     <g id=\"line2d_29\">\n      <path d=\"M 571.761635 101.518125 \nL 571.761635 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_31\">\n      <!-- 9 -->\n      <g style=\"fill: #262626\" transform=\"translate(568.580385 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-39\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_26\">\n     <g id=\"line2d_30\">\n      <path d=\"M 590.207916 101.518125 \nL 590.207916 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_32\">\n      <!-- 10 -->\n      <g style=\"fill: #262626\" transform=\"translate(583.845416 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_27\">\n     <g id=\"line2d_31\">\n      <path d=\"M 608.654197 101.518125 \nL 608.654197 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_33\">\n      <!-- 11 -->\n      <g style=\"fill: #262626\" transform=\"translate(602.291697 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_28\">\n     <g id=\"line2d_32\">\n      <path d=\"M 627.100478 101.518125 \nL 627.100478 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_34\">\n      <!-- 12 -->\n      <g style=\"fill: #262626\" transform=\"translate(620.737978 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_29\">\n     <g id=\"line2d_33\">\n      <path d=\"M 645.546759 101.518125 \nL 645.546759 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_35\">\n      <!-- 13 -->\n      <g style=\"fill: #262626\" transform=\"translate(639.184259 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_30\">\n     <g id=\"line2d_34\">\n      <path d=\"M 663.99304 101.518125 \nL 663.99304 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_36\">\n      <!-- 14 -->\n      <g style=\"fill: #262626\" transform=\"translate(657.63054 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_31\">\n     <g id=\"line2d_35\">\n      <path d=\"M 682.439321 101.518125 \nL 682.439321 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_37\">\n      <!-- 15 -->\n      <g style=\"fill: #262626\" transform=\"translate(676.076821 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_32\">\n     <g id=\"line2d_36\">\n      <path d=\"M 700.885602 101.518125 \nL 700.885602 22.318125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_38\">\n      <!-- 16 -->\n      <g style=\"fill: #262626\" transform=\"translate(694.523102 118.616563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_39\">\n     <!-- Position in sequence -->\n     <g style=\"fill: #262626\" transform=\"translate(511.243182 132.294688) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_4\">\n     <g id=\"line2d_37\">\n      <path d=\"M 410.356676 94.918125 \nL 714.720312 94.918125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_40\">\n      <!-- −1 -->\n      <g style=\"fill: #262626\" transform=\"translate(386.114489 98.717344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_38\">\n      <path d=\"M 410.356676 61.918125 \nL 714.720312 61.918125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_41\">\n      <!-- 0 -->\n      <g style=\"fill: #262626\" transform=\"translate(394.494176 65.717344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_39\">\n      <path d=\"M 410.356676 28.918125 \nL 714.720312 28.918125 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_42\">\n      <!-- 1 -->\n      <g style=\"fill: #262626\" transform=\"translate(394.494176 32.717344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_43\">\n     <!-- Positional encoding -->\n     <g style=\"fill: #262626\" transform=\"translate(380.034801 110.384531) rotate(-90) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_40\">\n    <path d=\"M 424.191387 28.918125 \nL 442.637668 44.088148 \nL 461.083949 75.650971 \nL 479.53023 94.587877 \nL 497.976511 83.488364 \nL 516.422792 52.557272 \nL 534.869073 30.232506 \nL 553.315354 37.039351 \nL 571.761635 66.719626 \nL 590.207916 91.985423 \nL 608.654197 89.607485 \nL 627.100478 61.772077 \nL 645.546759 34.070945 \nL 663.99304 31.972381 \nL 682.439321 57.405797 \nL 700.885602 86.987826 \n\" clip-path=\"url(#p167a713cd6)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: round\"/>\n    <defs>\n     <path id=\"mc6ff58947c\" d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" style=\"stroke: #000000\"/>\n    </defs>\n    <g clip-path=\"url(#p167a713cd6)\">\n     <use xlink:href=\"#mc6ff58947c\" x=\"424.191387\" y=\"28.918125\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"442.637668\" y=\"44.088148\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"461.083949\" y=\"75.650971\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"479.53023\" y=\"94.587877\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"497.976511\" y=\"83.488364\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"516.422792\" y=\"52.557272\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"534.869073\" y=\"30.232506\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"553.315354\" y=\"37.039351\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"571.761635\" y=\"66.719626\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"590.207916\" y=\"91.985423\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"608.654197\" y=\"89.607485\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"627.100478\" y=\"61.772077\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"645.546759\" y=\"34.070945\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"663.99304\" y=\"31.972381\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"682.439321\" y=\"57.405797\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n     <use xlink:href=\"#mc6ff58947c\" x=\"700.885602\" y=\"86.987826\" style=\"fill: #ff7f0e; stroke: #000000\"/>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 410.356676 101.518125 \nL 410.356676 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 714.720312 101.518125 \nL 714.720312 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 410.356676 101.518125 \nL 714.720312 101.518125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 410.356676 22.318125 \nL 714.720312 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_44\">\n    <!-- Encoding in hidden dimension 2 -->\n    <g style=\"fill: #262626\" transform=\"translate(466.309744 16.318125) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-45\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n     <use xlink:href=\"#DejaVuSans-63\" x=\"126.5625\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"181.542969\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"242.724609\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"306.201172\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"333.984375\"/>\n     <use xlink:href=\"#DejaVuSans-67\" x=\"397.363281\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"460.839844\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"492.626953\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"520.410156\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"583.789062\"/>\n     <use xlink:href=\"#DejaVuSans-68\" x=\"615.576172\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"678.955078\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"706.738281\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"770.214844\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"833.691406\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"895.214844\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"958.59375\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"990.380859\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1053.857422\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"1081.640625\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1179.052734\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1240.576172\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"1303.955078\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1356.054688\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"1383.837891\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1445.019531\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"1508.398438\"/>\n     <use xlink:href=\"#DejaVuSans-32\" x=\"1540.185547\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 45.120313 244.078125 \nL 349.483949 244.078125 \nL 349.483949 164.878125 \nL 45.120313 164.878125 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_5\">\n    <g id=\"xtick_33\">\n     <g id=\"line2d_41\">\n      <path d=\"M 58.955023 244.078125 \nL 58.955023 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_45\">\n      <!-- 1 -->\n      <g style=\"fill: #262626\" transform=\"translate(55.773773 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_34\">\n     <g id=\"line2d_42\">\n      <path d=\"M 77.401304 244.078125 \nL 77.401304 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_46\">\n      <!-- 2 -->\n      <g style=\"fill: #262626\" transform=\"translate(74.220054 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_35\">\n     <g id=\"line2d_43\">\n      <path d=\"M 95.847585 244.078125 \nL 95.847585 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_47\">\n      <!-- 3 -->\n      <g style=\"fill: #262626\" transform=\"translate(92.666335 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_36\">\n     <g id=\"line2d_44\">\n      <path d=\"M 114.293866 244.078125 \nL 114.293866 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_48\">\n      <!-- 4 -->\n      <g style=\"fill: #262626\" transform=\"translate(111.112616 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_37\">\n     <g id=\"line2d_45\">\n      <path d=\"M 132.740147 244.078125 \nL 132.740147 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_49\">\n      <!-- 5 -->\n      <g style=\"fill: #262626\" transform=\"translate(129.558897 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_38\">\n     <g id=\"line2d_46\">\n      <path d=\"M 151.186428 244.078125 \nL 151.186428 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_50\">\n      <!-- 6 -->\n      <g style=\"fill: #262626\" transform=\"translate(148.005178 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_39\">\n     <g id=\"line2d_47\">\n      <path d=\"M 169.632709 244.078125 \nL 169.632709 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_51\">\n      <!-- 7 -->\n      <g style=\"fill: #262626\" transform=\"translate(166.451459 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-37\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_40\">\n     <g id=\"line2d_48\">\n      <path d=\"M 188.07899 244.078125 \nL 188.07899 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_52\">\n      <!-- 8 -->\n      <g style=\"fill: #262626\" transform=\"translate(184.89774 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_41\">\n     <g id=\"line2d_49\">\n      <path d=\"M 206.525271 244.078125 \nL 206.525271 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_53\">\n      <!-- 9 -->\n      <g style=\"fill: #262626\" transform=\"translate(203.344021 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-39\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_42\">\n     <g id=\"line2d_50\">\n      <path d=\"M 224.971552 244.078125 \nL 224.971552 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_54\">\n      <!-- 10 -->\n      <g style=\"fill: #262626\" transform=\"translate(218.609052 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_43\">\n     <g id=\"line2d_51\">\n      <path d=\"M 243.417833 244.078125 \nL 243.417833 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_55\">\n      <!-- 11 -->\n      <g style=\"fill: #262626\" transform=\"translate(237.055333 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_44\">\n     <g id=\"line2d_52\">\n      <path d=\"M 261.864114 244.078125 \nL 261.864114 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_56\">\n      <!-- 12 -->\n      <g style=\"fill: #262626\" transform=\"translate(255.501614 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_45\">\n     <g id=\"line2d_53\">\n      <path d=\"M 280.310395 244.078125 \nL 280.310395 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_57\">\n      <!-- 13 -->\n      <g style=\"fill: #262626\" transform=\"translate(273.947895 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_46\">\n     <g id=\"line2d_54\">\n      <path d=\"M 298.756676 244.078125 \nL 298.756676 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_58\">\n      <!-- 14 -->\n      <g style=\"fill: #262626\" transform=\"translate(292.394176 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_47\">\n     <g id=\"line2d_55\">\n      <path d=\"M 317.202957 244.078125 \nL 317.202957 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_59\">\n      <!-- 15 -->\n      <g style=\"fill: #262626\" transform=\"translate(310.840457 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_48\">\n     <g id=\"line2d_56\">\n      <path d=\"M 335.649238 244.078125 \nL 335.649238 164.878125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_60\">\n      <!-- 16 -->\n      <g style=\"fill: #262626\" transform=\"translate(329.286738 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_61\">\n     <!-- Position in sequence -->\n     <g style=\"fill: #262626\" transform=\"translate(146.006818 274.854688) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_6\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_57\">\n      <path d=\"M 45.120313 237.478125 \nL 349.483949 237.478125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_62\">\n      <!-- −1 -->\n      <g style=\"fill: #262626\" transform=\"translate(20.878125 241.277344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_58\">\n      <path d=\"M 45.120313 204.478125 \nL 349.483949 204.478125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_63\">\n      <!-- 0 -->\n      <g style=\"fill: #262626\" transform=\"translate(29.257813 208.277344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_59\">\n      <path d=\"M 45.120313 171.478125 \nL 349.483949 171.478125 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_64\">\n      <!-- 1 -->\n      <g style=\"fill: #262626\" transform=\"translate(29.257813 175.277344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_65\">\n     <!-- Positional encoding -->\n     <g style=\"fill: #262626\" transform=\"translate(14.798438 252.944531) rotate(-90) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_60\">\n    <path d=\"M 58.955023 204.478125 \nL 77.401304 183.694819 \nL 95.847585 172.190858 \nL 114.293866 175.102539 \nL 132.740147 191.129855 \nL 151.186428 213.116915 \nL 169.632709 231.246921 \nL 188.07899 237.425177 \nL 206.525271 228.893208 \nL 224.971552 209.460375 \nL 243.417833 187.803062 \nL 261.864114 173.590845 \nL 280.310395 173.169203 \nL 298.756676 186.726378 \nL 317.202957 208.209386 \nL 335.649238 228.026456 \n\" clip-path=\"url(#p6a87397199)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: round\"/>\n    <defs>\n     <path id=\"m5e7ace56fd\" d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" style=\"stroke: #000000\"/>\n    </defs>\n    <g clip-path=\"url(#p6a87397199)\">\n     <use xlink:href=\"#m5e7ace56fd\" x=\"58.955023\" y=\"204.478125\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"77.401304\" y=\"183.694819\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"95.847585\" y=\"172.190858\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"114.293866\" y=\"175.102539\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"132.740147\" y=\"191.129855\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"151.186428\" y=\"213.116915\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"169.632709\" y=\"231.246921\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"188.07899\" y=\"237.425177\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"206.525271\" y=\"228.893208\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"224.971552\" y=\"209.460375\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"243.417833\" y=\"187.803062\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"261.864114\" y=\"173.590845\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"280.310395\" y=\"173.169203\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"298.756676\" y=\"186.726378\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"317.202957\" y=\"208.209386\" style=\"fill: #2ca02c; stroke: #000000\"/>\n     <use xlink:href=\"#m5e7ace56fd\" x=\"335.649238\" y=\"228.026456\" style=\"fill: #2ca02c; stroke: #000000\"/>\n    </g>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 45.120313 244.078125 \nL 45.120313 164.878125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 349.483949 244.078125 \nL 349.483949 164.878125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 45.120313 244.078125 \nL 349.483949 244.078125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 45.120313 164.878125 \nL 349.483949 164.878125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_66\">\n    <!-- Encoding in hidden dimension 3 -->\n    <g style=\"fill: #262626\" transform=\"translate(101.073381 158.878125) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-45\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n     <use xlink:href=\"#DejaVuSans-63\" x=\"126.5625\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"181.542969\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"242.724609\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"306.201172\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"333.984375\"/>\n     <use xlink:href=\"#DejaVuSans-67\" x=\"397.363281\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"460.839844\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"492.626953\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"520.410156\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"583.789062\"/>\n     <use xlink:href=\"#DejaVuSans-68\" x=\"615.576172\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"678.955078\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"706.738281\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"770.214844\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"833.691406\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"895.214844\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"958.59375\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"990.380859\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1053.857422\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"1081.640625\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1179.052734\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1240.576172\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"1303.955078\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1356.054688\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"1383.837891\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1445.019531\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"1508.398438\"/>\n     <use xlink:href=\"#DejaVuSans-33\" x=\"1540.185547\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g id=\"patch_17\">\n    <path d=\"M 410.356676 244.078125 \nL 714.720312 244.078125 \nL 714.720312 164.878125 \nL 410.356676 164.878125 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_7\">\n    <g id=\"xtick_49\">\n     <g id=\"line2d_61\">\n      <path d=\"M 424.191387 244.078125 \nL 424.191387 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_67\">\n      <!-- 1 -->\n      <g style=\"fill: #262626\" transform=\"translate(421.010137 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_50\">\n     <g id=\"line2d_62\">\n      <path d=\"M 442.637668 244.078125 \nL 442.637668 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_68\">\n      <!-- 2 -->\n      <g style=\"fill: #262626\" transform=\"translate(439.456418 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_51\">\n     <g id=\"line2d_63\">\n      <path d=\"M 461.083949 244.078125 \nL 461.083949 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_69\">\n      <!-- 3 -->\n      <g style=\"fill: #262626\" transform=\"translate(457.902699 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_52\">\n     <g id=\"line2d_64\">\n      <path d=\"M 479.53023 244.078125 \nL 479.53023 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_70\">\n      <!-- 4 -->\n      <g style=\"fill: #262626\" transform=\"translate(476.34898 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_53\">\n     <g id=\"line2d_65\">\n      <path d=\"M 497.976511 244.078125 \nL 497.976511 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_71\">\n      <!-- 5 -->\n      <g style=\"fill: #262626\" transform=\"translate(494.795261 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_54\">\n     <g id=\"line2d_66\">\n      <path d=\"M 516.422792 244.078125 \nL 516.422792 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_72\">\n      <!-- 6 -->\n      <g style=\"fill: #262626\" transform=\"translate(513.241542 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_55\">\n     <g id=\"line2d_67\">\n      <path d=\"M 534.869073 244.078125 \nL 534.869073 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_73\">\n      <!-- 7 -->\n      <g style=\"fill: #262626\" transform=\"translate(531.687823 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-37\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_56\">\n     <g id=\"line2d_68\">\n      <path d=\"M 553.315354 244.078125 \nL 553.315354 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_74\">\n      <!-- 8 -->\n      <g style=\"fill: #262626\" transform=\"translate(550.134104 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_57\">\n     <g id=\"line2d_69\">\n      <path d=\"M 571.761635 244.078125 \nL 571.761635 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_75\">\n      <!-- 9 -->\n      <g style=\"fill: #262626\" transform=\"translate(568.580385 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-39\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_58\">\n     <g id=\"line2d_70\">\n      <path d=\"M 590.207916 244.078125 \nL 590.207916 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_76\">\n      <!-- 10 -->\n      <g style=\"fill: #262626\" transform=\"translate(583.845416 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_59\">\n     <g id=\"line2d_71\">\n      <path d=\"M 608.654197 244.078125 \nL 608.654197 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_77\">\n      <!-- 11 -->\n      <g style=\"fill: #262626\" transform=\"translate(602.291697 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_60\">\n     <g id=\"line2d_72\">\n      <path d=\"M 627.100478 244.078125 \nL 627.100478 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_78\">\n      <!-- 12 -->\n      <g style=\"fill: #262626\" transform=\"translate(620.737978 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_61\">\n     <g id=\"line2d_73\">\n      <path d=\"M 645.546759 244.078125 \nL 645.546759 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_79\">\n      <!-- 13 -->\n      <g style=\"fill: #262626\" transform=\"translate(639.184259 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_62\">\n     <g id=\"line2d_74\">\n      <path d=\"M 663.99304 244.078125 \nL 663.99304 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_80\">\n      <!-- 14 -->\n      <g style=\"fill: #262626\" transform=\"translate(657.63054 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_63\">\n     <g id=\"line2d_75\">\n      <path d=\"M 682.439321 244.078125 \nL 682.439321 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_81\">\n      <!-- 15 -->\n      <g style=\"fill: #262626\" transform=\"translate(676.076821 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_64\">\n     <g id=\"line2d_76\">\n      <path d=\"M 700.885602 244.078125 \nL 700.885602 164.878125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_82\">\n      <!-- 16 -->\n      <g style=\"fill: #262626\" transform=\"translate(694.523102 261.176563) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_83\">\n     <!-- Position in sequence -->\n     <g style=\"fill: #262626\" transform=\"translate(511.243182 274.854688) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"389.294922\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"421.082031\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"448.865234\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"512.244141\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"544.03125\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"596.130859\"/>\n      <use xlink:href=\"#DejaVuSans-71\" x=\"657.654297\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"721.130859\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"784.509766\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"846.033203\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"909.412109\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"964.392578\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_8\">\n    <g id=\"ytick_10\">\n     <g id=\"line2d_77\">\n      <path d=\"M 410.356676 237.478125 \nL 714.720312 237.478125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_84\">\n      <!-- −1 -->\n      <g style=\"fill: #262626\" transform=\"translate(386.114489 241.277344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_78\">\n      <path d=\"M 410.356676 204.478125 \nL 714.720312 204.478125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_85\">\n      <!-- 0 -->\n      <g style=\"fill: #262626\" transform=\"translate(394.494176 208.277344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_79\">\n      <path d=\"M 410.356676 171.478125 \nL 714.720312 171.478125 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_86\">\n      <!-- 1 -->\n      <g style=\"fill: #262626\" transform=\"translate(394.494176 175.277344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_87\">\n     <!-- Positional encoding -->\n     <g style=\"fill: #262626\" transform=\"translate(380.034801 252.944531) rotate(-90) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"117.859375\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"169.958984\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"197.742188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"236.951172\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"264.734375\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"325.916016\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"389.294922\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"450.574219\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"478.357422\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"510.144531\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"571.667969\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"635.046875\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"690.027344\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"751.208984\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"814.685547\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"842.46875\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"905.847656\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_80\">\n    <path d=\"M 424.191387 171.478125 \nL 442.637668 178.845057 \nL 461.083949 197.65666 \nL 479.53023 219.513916 \nL 497.976511 234.65798 \nL 516.422792 236.327322 \nL 534.869073 223.77661 \nL 553.315354 202.609496 \nL 571.761635 182.276691 \nL 590.207916 171.856396 \nL 608.654197 176.001077 \nL 627.100478 192.860209 \nL 645.546759 214.906519 \nL 663.99304 232.296741 \nL 682.439321 237.266502 \nL 700.885602 227.596864 \n\" clip-path=\"url(#pcaa483b77f)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: round\"/>\n    <defs>\n     <path id=\"m4bf27d102b\" d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" style=\"stroke: #000000\"/>\n    </defs>\n    <g clip-path=\"url(#pcaa483b77f)\">\n     <use xlink:href=\"#m4bf27d102b\" x=\"424.191387\" y=\"171.478125\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"442.637668\" y=\"178.845057\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"461.083949\" y=\"197.65666\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"479.53023\" y=\"219.513916\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"497.976511\" y=\"234.65798\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"516.422792\" y=\"236.327322\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"534.869073\" y=\"223.77661\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"553.315354\" y=\"202.609496\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"571.761635\" y=\"182.276691\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"590.207916\" y=\"171.856396\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"608.654197\" y=\"176.001077\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"627.100478\" y=\"192.860209\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"645.546759\" y=\"214.906519\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"663.99304\" y=\"232.296741\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"682.439321\" y=\"237.266502\" style=\"fill: #d62728; stroke: #000000\"/>\n     <use xlink:href=\"#m4bf27d102b\" x=\"700.885602\" y=\"227.596864\" style=\"fill: #d62728; stroke: #000000\"/>\n    </g>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 410.356676 244.078125 \nL 410.356676 164.878125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 714.720312 244.078125 \nL 714.720312 164.878125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 410.356676 244.078125 \nL 714.720312 244.078125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 410.356676 164.878125 \nL 714.720312 164.878125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_88\">\n    <!-- Encoding in hidden dimension 4 -->\n    <g style=\"fill: #262626\" transform=\"translate(466.309744 158.878125) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-45\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n     <use xlink:href=\"#DejaVuSans-63\" x=\"126.5625\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"181.542969\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"242.724609\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"306.201172\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"333.984375\"/>\n     <use xlink:href=\"#DejaVuSans-67\" x=\"397.363281\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"460.839844\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"492.626953\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"520.410156\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"583.789062\"/>\n     <use xlink:href=\"#DejaVuSans-68\" x=\"615.576172\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"678.955078\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"706.738281\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"770.214844\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"833.691406\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"895.214844\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"958.59375\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"990.380859\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1053.857422\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"1081.640625\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1179.052734\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1240.576172\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"1303.955078\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1356.054688\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"1383.837891\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1445.019531\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"1508.398438\"/>\n     <use xlink:href=\"#DejaVuSans-34\" x=\"1540.185547\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p009c6d3ebf\">\n   <rect x=\"45.120313\" y=\"22.318125\" width=\"304.363636\" height=\"79.2\"/>\n  </clipPath>\n  <clipPath id=\"p167a713cd6\">\n   <rect x=\"410.356676\" y=\"22.318125\" width=\"304.363636\" height=\"79.2\"/>\n  </clipPath>\n  <clipPath id=\"p6a87397199\">\n   <rect x=\"45.120313\" y=\"164.878125\" width=\"304.363636\" height=\"79.2\"/>\n  </clipPath>\n  <clipPath id=\"pcaa483b77f\">\n   <rect x=\"410.356676\" y=\"164.878125\" width=\"304.363636\" height=\"79.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "application/pdf": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.set_theme()\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12,4))\n",
        "ax = [a for a_list in ax for a in a_list]\n",
        "for i in range(len(ax)):\n",
        "    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n",
        "    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n",
        "    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n",
        "    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n",
        "    ax[i].set_xticks(np.arange(1,17))\n",
        "    ax[i].tick_params(axis='both', which='major', labelsize=10)\n",
        "    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n",
        "    ax[i].set_ylim(-1.2, 1.2)\n",
        "fig.subplots_adjust(hspace=0.8)\n",
        "sns.reset_orig()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ2TLJMMnTTF"
      },
      "source": [
        "As we can see, the patterns between the hidden dimension $1$ and $2$ only differ in the starting angle. The wavelength is $2\\pi$, hence the repetition after position $6$. The hidden dimensions $2$ and $3$ have about twice the wavelength."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsesYOHanTTF"
      },
      "source": [
        "### Learning rate warm-up\n",
        "\n",
        "One commonly used technique for training a Transformer is learning rate warm-up. This means that we gradually increase the learning rate from 0 on to our originally specified learning rate in the first few iterations. Thus, we slowly start learning instead of taking very large steps from the beginning. In fact, training a deep Transformer without learning rate warm-up can make the model diverge and achieve a much worse performance on training and testing. Take for instance the following plot by [Liu et al. (2019)](https://arxiv.org/pdf/1908.03265.pdf) comparing Adam-vanilla (i.e. Adam without warm-up) vs Adam with a warm-up:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/warmup_loss_plot.svg?raw=1\" width=\"350px\"></center>\n",
        "\n",
        "Clearly, the warm-up is a crucial hyperparameter in the Transformer architecture. Why is it so important? There are currently two common explanations. Firstly, Adam uses the bias correction factors which however can lead to a higher variance in the adaptive learning rate during the first iterations. Improved optimizers like [RAdam](https://arxiv.org/abs/1908.03265) have been shown to overcome this issue, not requiring warm-up for training Transformers. Secondly, the iteratively applied Layer Normalization across layers can lead to very high gradients during the first iterations, which can be solved by using [Pre-Layer Normalization](https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf) (similar to Pre-Activation ResNet), or replacing Layer Normalization by other techniques ([Adaptive Normalization](https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf), [Power Normalization](https://arxiv.org/abs/2003.07845)).\n",
        "\n",
        "Nevertheless, many applications and papers still use the original Transformer architecture with Adam, because warm-up is a simple, yet effective way of solving the gradient problem in the first iterations. There are many different schedulers we could use. For instance, the original Transformer paper used an exponential decay scheduler with a warm-up. However, the currently most popular scheduler is the cosine warm-up scheduler, which combines warm-up with a cosine-shaped learning rate decay. We can implement it below, and visualize the learning rate factor over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JbodzuopnTTG"
      },
      "outputs": [],
      "source": [
        "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
        "\n",
        "    def __init__(self, optimizer, warmup, max_iters):\n",
        "        self.warmup = warmup\n",
        "        self.max_num_iters = max_iters\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
        "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
        "\n",
        "    def get_lr_factor(self, epoch):\n",
        "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
        "        if epoch <= self.warmup:\n",
        "            lr_factor *= epoch * 1.0 / self.warmup\n",
        "        return lr_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "6lI8nsCInTTG",
        "outputId": "b15a84d3-aeac-4908-d1e1-577d1e34fc9c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"503.407187pt\" height=\"231.597813pt\" viewBox=\"0 0 503.407187 231.597813\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-02-26T18:12:39.697762</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 231.597813 \nL 503.407187 231.597813 \nL 503.407187 -0 \nL 0 -0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 49.807188 188.638125 \nL 496.207187 188.638125 \nL 496.207187 22.318125 \nL 49.807188 22.318125 \nz\n\" style=\"fill: #eaeaf2\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 70.098097 188.638125 \nL 70.098097 22.318125 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g style=\"fill: #262626\" transform=\"translate(66.598722 206.496406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path d=\"M 120.850746 188.638125 \nL 120.850746 22.318125 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 250 -->\n      <g style=\"fill: #262626\" transform=\"translate(110.352621 206.496406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path d=\"M 171.603395 188.638125 \nL 171.603395 22.318125 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 500 -->\n      <g style=\"fill: #262626\" transform=\"translate(161.10527 206.496406) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path d=\"M 222.356044 188.638125 \nL 222.356044 22.318125 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 750 -->\n      <g style=\"fill: #262626\" transform=\"translate(211.857919 206.496406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path d=\"M 273.108693 188.638125 \nL 273.108693 22.318125 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1000 -->\n      <g style=\"fill: #262626\" transform=\"translate(259.111193 206.496406) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path d=\"M 323.861342 188.638125 \nL 323.861342 22.318125 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1250 -->\n      <g style=\"fill: #262626\" transform=\"translate(309.863842 206.496406) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <path d=\"M 374.613991 188.638125 \nL 374.613991 22.318125 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1500 -->\n      <g style=\"fill: #262626\" transform=\"translate(360.616491 206.496406) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <path d=\"M 425.36664 188.638125 \nL 425.36664 22.318125 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1750 -->\n      <g style=\"fill: #262626\" transform=\"translate(411.36914 206.496406) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <path d=\"M 476.119289 188.638125 \nL 476.119289 22.318125 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2000 -->\n      <g style=\"fill: #262626\" transform=\"translate(462.121789 206.496406) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- Iterations (in batches) -->\n     <g style=\"fill: #262626\" transform=\"translate(206.709062 221.902188) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-49\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-49\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"29.492188\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"68.701172\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"130.224609\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"171.337891\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"232.617188\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"271.826172\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"299.609375\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"360.791016\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"424.169922\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"476.269531\"/>\n      <use xlink:href=\"#DejaVuSans-28\" x=\"508.056641\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"547.070312\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"574.853516\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"638.232422\"/>\n      <use xlink:href=\"#DejaVuSans-62\" x=\"670.019531\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"733.496094\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"794.775391\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"833.984375\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"888.964844\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"952.34375\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"1013.867188\"/>\n      <use xlink:href=\"#DejaVuSans-29\" x=\"1065.966797\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <path d=\"M 49.807188 181.078125 \nL 496.207187 181.078125 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.0 -->\n      <g style=\"fill: #262626\" transform=\"translate(22.81375 185.257266) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path d=\"M 49.807188 150.65082 \nL 496.207187 150.65082 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.2 -->\n      <g style=\"fill: #262626\" transform=\"translate(22.81375 154.82996) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <path d=\"M 49.807188 120.223514 \nL 496.207187 120.223514 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.4 -->\n      <g style=\"fill: #262626\" transform=\"translate(22.81375 124.402655) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <path d=\"M 49.807188 89.796209 \nL 496.207187 89.796209 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.6 -->\n      <g style=\"fill: #262626\" transform=\"translate(22.81375 93.97535) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <path d=\"M 49.807188 59.368904 \nL 496.207187 59.368904 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.8 -->\n      <g style=\"fill: #262626\" transform=\"translate(22.81375 63.548044) scale(0.11 -0.11)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <path d=\"M 49.807188 28.941598 \nL 496.207187 28.941598 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #ffffff; stroke-linecap: round\"/>\n     </g>\n     <g id=\"text_16\">\n      <!-- 1.0 -->\n      <g style=\"fill: #262626\" transform=\"translate(22.81375 33.120739) scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- Learning rate factor -->\n     <g style=\"fill: #262626\" transform=\"translate(16.318125 165.106875) rotate(-90) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"53.962891\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"115.486328\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"176.765625\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"216.128906\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"279.507812\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"307.291016\"/>\n      <use xlink:href=\"#DejaVuSans-67\" x=\"370.669922\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"434.146484\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"465.933594\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"507.046875\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"568.326172\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"607.535156\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"669.058594\"/>\n      <use xlink:href=\"#DejaVuSans-66\" x=\"700.845703\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"736.050781\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"797.330078\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"852.310547\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"891.519531\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"952.701172\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 70.098097 181.078125 \nL 89.587114 35.855772 \nL 90.399156 29.878125 \nL 97.504527 30.645532 \nL 104.609898 31.637654 \nL 111.715269 32.851492 \nL 119.02365 34.327453 \nL 126.332032 36.029378 \nL 133.843424 38.008318 \nL 141.557826 40.276189 \nL 149.272229 42.774634 \nL 157.189642 45.56924 \nL 165.513077 48.747409 \nL 174.039522 52.244859 \nL 182.971988 56.154818 \nL 192.310476 60.491614 \nL 202.054984 65.264303 \nL 212.611535 70.688692 \nL 223.980129 76.785211 \nL 236.769796 83.902262 \nL 252.198601 92.756247 \nL 275.950841 106.682555 \nL 301.327166 121.487009 \nL 316.146939 129.875666 \nL 328.733596 136.74803 \nL 339.899179 142.594705 \nL 350.252719 147.766568 \nL 359.997228 152.384247 \nL 369.132705 156.469316 \nL 377.86216 160.132967 \nL 386.388605 163.468848 \nL 394.509029 166.40961 \nL 402.426443 169.043634 \nL 410.140845 171.379118 \nL 417.652237 173.426057 \nL 425.163629 175.241961 \nL 432.472011 176.781193 \nL 439.780392 178.090981 \nL 446.885763 179.140428 \nL 453.991134 179.965868 \nL 461.096505 180.564806 \nL 468.201876 180.935431 \nL 475.104236 181.075779 \nL 475.916278 181.078031 \nL 475.916278 181.078031 \n\" clip-path=\"url(#p1c2146dd9e)\" style=\"fill: none; stroke: #4c72b0; stroke-width: 1.5; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 49.807188 188.638125 \nL 49.807188 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 496.207187 188.638125 \nL 496.207187 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 49.807187 188.638125 \nL 496.207188 188.638125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 49.807187 22.318125 \nL 496.207188 22.318125 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_18\">\n    <!-- Cosine Warm-up Learning Rate Scheduler -->\n    <g style=\"fill: #262626\" transform=\"translate(148.179062 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-43\" d=\"M 4122 4306 \nL 4122 3641 \nQ 3803 3938 3442 4084 \nQ 3081 4231 2675 4231 \nQ 1875 4231 1450 3742 \nQ 1025 3253 1025 2328 \nQ 1025 1406 1450 917 \nQ 1875 428 2675 428 \nQ 3081 428 3442 575 \nQ 3803 722 4122 1019 \nL 4122 359 \nQ 3791 134 3420 21 \nQ 3050 -91 2638 -91 \nQ 1578 -91 968 557 \nQ 359 1206 359 2328 \nQ 359 3453 968 4101 \nQ 1578 4750 2638 4750 \nQ 3056 4750 3426 4639 \nQ 3797 4528 4122 4306 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-57\" d=\"M 213 4666 \nL 850 4666 \nL 1831 722 \nL 2809 4666 \nL 3519 4666 \nL 4500 722 \nL 5478 4666 \nL 6119 4666 \nL 4947 0 \nL 4153 0 \nL 3169 4050 \nL 2175 0 \nL 1381 0 \nL 213 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-52\" d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-53\" d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-43\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"69.824219\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"131.005859\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"183.105469\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"210.888672\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"274.267578\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"335.791016\"/>\n     <use xlink:href=\"#DejaVuSans-57\" x=\"367.578125\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"460.080078\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"521.359375\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"560.722656\"/>\n     <use xlink:href=\"#DejaVuSans-2d\" x=\"658.134766\"/>\n     <use xlink:href=\"#DejaVuSans-75\" x=\"694.21875\"/>\n     <use xlink:href=\"#DejaVuSans-70\" x=\"757.597656\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"821.074219\"/>\n     <use xlink:href=\"#DejaVuSans-4c\" x=\"852.861328\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"906.824219\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"968.347656\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"1029.626953\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1068.990234\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1132.369141\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1160.152344\"/>\n     <use xlink:href=\"#DejaVuSans-67\" x=\"1223.53125\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"1287.007812\"/>\n     <use xlink:href=\"#DejaVuSans-52\" x=\"1318.794922\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"1386.027344\"/>\n     <use xlink:href=\"#DejaVuSans-74\" x=\"1447.306641\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1486.515625\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"1548.039062\"/>\n     <use xlink:href=\"#DejaVuSans-53\" x=\"1579.826172\"/>\n     <use xlink:href=\"#DejaVuSans-63\" x=\"1643.302734\"/>\n     <use xlink:href=\"#DejaVuSans-68\" x=\"1698.283203\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1761.662109\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"1823.185547\"/>\n     <use xlink:href=\"#DejaVuSans-75\" x=\"1886.662109\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"1950.041016\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1977.824219\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"2039.347656\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p1c2146dd9e\">\n   <rect x=\"49.807188\" y=\"22.318125\" width=\"446.4\" height=\"166.32\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "application/pdf": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Needed for initializing the lr scheduler\n",
        "p = nn.Parameter(torch.empty(4,4))\n",
        "optimizer = optim.Adam([p], lr=1e-3)\n",
        "lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n",
        "\n",
        "# Plotting\n",
        "epochs = list(range(2000))\n",
        "sns.set()\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(epochs, [lr_scheduler.get_lr_factor(e) for e in epochs])\n",
        "plt.ylabel(\"Learning rate factor\")\n",
        "plt.xlabel(\"Iterations (in batches)\")\n",
        "plt.title(\"Cosine Warm-up Learning Rate Scheduler\")\n",
        "plt.show()\n",
        "sns.reset_orig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8K8wht2nTTG"
      },
      "source": [
        "In the first 100 iterations, we increase the learning rate factor from 0 to 1, whereas for all later iterations, we decay it using the cosine wave. Pre-implementations of this scheduler can be found in the popular NLP Transformer library [huggingface](https://huggingface.co/transformers/main_classes/optimizer_schedules.html?highlight=cosine#transformers.get_cosine_schedule_with_warmup)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSf2cQtrnTTG"
      },
      "source": [
        "### PyTorch Lightning Module\n",
        "\n",
        "Finally, we can embed the Transformer architecture into a PyTorch lightning module. From Tutorial 5, you know that PyTorch Lightning simplifies our training and test code, as well as structures the code nicely in separate functions. We will implement a template for a classifier based on the Transformer encoder. Thereby, we have a prediction output per sequence element. If we would need a classifier over the whole sequence, the common approach is to add an additional `[CLS]` token to the sequence, representing the classifier token. However, here we focus on tasks where we have an output per element.\n",
        "\n",
        "Additionally to the Transformer architecture, we add a small input network (maps input dimensions to model dimensions), the positional encoding, and an output network (transforms output encodings to predictions). We also add the learning rate scheduler, which takes a step each iteration instead of once per epoch. This is needed for the warmup and the smooth cosine decay. The training, validation, and test step is left empty for now and will be filled for our task-specific models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "X49FqGxPnTTG"
      },
      "outputs": [],
      "source": [
        "class TransformerPredictor(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim - Hidden dimensionality of the input\n",
        "            model_dim - Hidden dimensionality to use inside the Transformer\n",
        "            num_classes - Number of classes to predict per sequence element\n",
        "            num_heads - Number of heads to use in the Multi-Head Attention blocks\n",
        "            num_layers - Number of encoder blocks to use.\n",
        "            lr - Learning rate in the optimizer\n",
        "            warmup - Number of warmup steps. Usually between 50 and 500\n",
        "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
        "            dropout - Dropout to apply inside the model\n",
        "            input_dropout - Dropout to apply on the input features\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self._create_model()\n",
        "\n",
        "    def _create_model(self):\n",
        "        # Input dim -> Model dim\n",
        "        self.input_net = nn.Sequential(\n",
        "            nn.Dropout(self.hparams.input_dropout),\n",
        "            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n",
        "        )\n",
        "        # Positional encoding for sequences\n",
        "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n",
        "        # Transformer\n",
        "        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n",
        "                                              input_dim=self.hparams.model_dim,\n",
        "                                              dim_feedforward=2*self.hparams.model_dim,\n",
        "                                              num_heads=self.hparams.num_heads,\n",
        "                                              dropout=self.hparams.dropout)\n",
        "        # Output classifier per sequence lement\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
        "            nn.LayerNorm(self.hparams.model_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(self.hparams.dropout),\n",
        "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features of shape [Batch, SeqLen, input_dim]\n",
        "            mask - Mask to apply on the attention outputs (optional)\n",
        "            add_positional_encoding - If True, we add the positional encoding to the input.\n",
        "                                      Might not be desired for some tasks.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        x = self.transformer(x, mask=mask)\n",
        "        x = self.output_net(x)\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
        "        Input arguments same as the forward pass.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
        "        return attention_maps\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "\n",
        "        # Apply lr scheduler per step\n",
        "        lr_scheduler = CosineWarmupScheduler(optimizer,\n",
        "                                             warmup=self.hparams.warmup,\n",
        "                                             max_iters=self.hparams.max_iters)\n",
        "        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w29Y9mYBnTTH"
      },
      "source": [
        "## Experiments\n",
        "\n",
        "After having finished the implementation of the Transformer architecture, we can start experimenting and apply it to various tasks. In this notebook, we will focus on two tasks: parallel Sequence-to-Sequence, and set anomaly detection. The two tasks focus on different properties of the Transformer architecture, and we go through them below.\n",
        "\n",
        "### Sequence to Sequence\n",
        "\n",
        "A Sequence-to-Sequence task represents a task where the input _and_ the output is a sequence, not necessarily of the same length. Popular tasks in this domain include machine translation and summarization. For this, we usually have a Transformer encoder for interpreting the input sequence, and a decoder for generating the output in an autoregressive manner. Here, however, we will go back to a much simpler example task and use only the encoder. Given a sequence of $N$ numbers between $0$ and $M$, the task is to reverse the input sequence. In Numpy notation, if our input is $x$, the output should be $x$[::-1]. Although this task sounds very simple, RNNs can have issues with such because the task requires long-term dependencies. Transformers are built to support such, and hence, we expect it to perform very well.\n",
        "\n",
        "First, let's create a dataset class below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IY2KGHRbnTTH"
      },
      "outputs": [],
      "source": [
        "class ReverseDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, num_categories, seq_len, size):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "\n",
        "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_data = self.data[idx]\n",
        "        labels = torch.flip(inp_data, dims=(0,))\n",
        "        return inp_data, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsZxpQlpnTTH"
      },
      "source": [
        "We create an arbitrary number of random sequences of numbers between 0 and `num_categories-1`. The label is simply the tensor flipped over the sequence dimension. We can create the corresponding data loaders below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sZRHL0jznTTH"
      },
      "outputs": [],
      "source": [
        "dataset = partial(ReverseDataset, 10, 16)\n",
        "train_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
        "val_loader   = data.DataLoader(dataset(1000), batch_size=128)\n",
        "test_loader  = data.DataLoader(dataset(10000), batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rcpv-g6nTTH"
      },
      "source": [
        "Let's look at an arbitrary sample of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOsL58y-nTTN",
        "outputId": "85f7c544-c35a-4a48-bcd6-33c801a9c6eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data: tensor([9, 6, 2, 0, 6, 2, 7, 9, 7, 3, 3, 4, 3, 7, 0, 9])\n",
            "Labels:     tensor([9, 0, 7, 3, 4, 3, 3, 7, 9, 7, 2, 6, 0, 2, 6, 9])\n"
          ]
        }
      ],
      "source": [
        "inp_data, labels = train_loader.dataset[0]\n",
        "print(\"Input data:\", inp_data)\n",
        "print(\"Labels:    \", labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3R1HtomnTTN"
      },
      "source": [
        "During training, we pass the input sequence through the Transformer encoder and predict the output for each input token. We use the standard Cross-Entropy loss to perform this. Every number is represented as a one-hot vector. Remember that representing the categories as single scalars decreases the expressiveness of the model extremely as $0$ and $1$ are not closer related than $0$ and $9$ in our example. An alternative to a one-hot vector is using a learned embedding vector as it is provided by the PyTorch module `nn.Embedding`. However, using a one-hot vector with an additional linear layer as in our case has the same effect as an embedding layer (`self.input_net` maps one-hot vector to a dense vector, where each row of the weight matrix represents the embedding for a specific category).\n",
        "\n",
        "To implement the training dynamic, we create a new class inheriting from `TransformerPredictor` and overwriting the training, validation and test step functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FYM3kFP0nTTN"
      },
      "outputs": [],
      "source": [
        "class ReversePredictor(TransformerPredictor):\n",
        "\n",
        "    def _calculate_loss(self, batch, mode=\"train\"):\n",
        "        # Fetch data and transform categories to one-hot vectors\n",
        "        inp_data, labels = batch\n",
        "        inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n",
        "\n",
        "        # Perform prediction and calculate loss and accuracy\n",
        "        preds = self.forward(inp_data, add_positional_encoding=True)\n",
        "        loss = F.cross_entropy(preds.view(-1,preds.size(-1)), labels.view(-1))\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        # Logging\n",
        "        self.log(f\"{mode}_loss\", loss)\n",
        "        self.log(f\"{mode}_acc\", acc)\n",
        "        return loss, acc\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2C9qPS_nTTN"
      },
      "source": [
        "Finally, we can create a training function similar to the one we have seen in Tutorial 5 for PyTorch Lightning. We create a `pl.Trainer` object, running for $N$ epochs, logging in TensorBoard, and saving our best model based on the validation. Afterward, we test our models on the test set. An additional parameter we pass to the trainer here is `gradient_clip_val`. This clips the norm of the gradients for all parameters before taking an optimizer step and prevents the model from diverging if we obtain very high gradients at, for instance, sharp loss surfaces (see many good blog posts on gradient clipping, like [DeepAI glossary](https://deepai.org/machine-learning-glossary-and-terms/gradient-clipping)). For Transformers, gradient clipping can help to further stabilize the training during the first few iterations, and also afterward. In plain PyTorch, you can apply gradient clipping via `torch.nn.utils.clip_grad_norm_(...)` (see [documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_)). The clip value is usually between 0.5 and 10, depending on how harsh you want to clip large gradients. After having explained this, let's implement the training function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-UFgkwr6nTTO"
      },
      "outputs": [],
      "source": [
        "def train_reverse(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"ReverseTask\")\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=10,\n",
        "                         gradient_clip_val=5)\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ReverseTask.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = ReversePredictor.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        model = ReversePredictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "    # Test best model on validation and test set\n",
        "    val_result = trainer.test(model, val_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTVDiCcenTTO"
      },
      "source": [
        "Finally, we can train the model. In this setup, we will use a single encoder block and a single head in the Multi-Head Attention. This is chosen because of the simplicity of the task, and in this case, the attention can actually be interpreted as an \"explanation\" of the predictions (compared to the other papers above dealing with deep Transformers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "6b96b702dcb142c3b81000db367cc85f",
            "d83ad437a42e412f8b787bdefa7b79ae",
            "0436637c8d95419a8f3f07875a8cd975",
            "9a0564266c534d159f31696e98fde9b4",
            "b14c7e8218be4c72b4ebab4fe8da8489",
            "12368f3f029a474d8e8236274fce948f",
            "4bea47bf280949c6af21d04929fe5445",
            "fea821813b2e492eb32f60b3b5bb6596",
            "0d6cba062aa247bc91e2d5f43a5c5fd0",
            "c7466b3b5a01436c805347f39fe70a99",
            "b4123d11e31c4fddbb55cd3339c91a76",
            "f1a24b0a638143d8aec3a439394994c9",
            "8f69435bbbb046be929f6047a35d4c1d",
            "858431d99a35482dad1c7d9f309739fd",
            "4c9bab5bf0244e4ba4fdc5b4e713fb1f",
            "c9369a156d1047c6a3823577f00bfdbc",
            "5919278278024223851fd7a0c9a79377",
            "22ca997ab9584f6db2ab39c2a5e49674",
            "cc9a90857516499b9c6ef7a9ea84a245",
            "c3771c3751ab465c8e4374af0865019c",
            "eb04ccdf4a2a4b6b8bb279ffec0123be",
            "14e5acbc13744710928b579c14d641ed"
          ]
        },
        "id": "DSCCFew0nTTO",
        "outputId": "1d984ea5-10d1-4d4e-e66d-b33b6150f661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found pretrained model, loading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.2.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../saved_models/tutorial6/ReverseTask.ckpt`\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: ../saved_models/tutorial6/ReverseTask/lightning_logs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b96b702dcb142c3b81000db367cc85f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1a24b0a638143d8aec3a439394994c9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "reverse_model, reverse_result = train_reverse(input_dim=train_loader.dataset.num_categories,\n",
        "                                              model_dim=32,\n",
        "                                              num_heads=1,\n",
        "                                              num_classes=train_loader.dataset.num_categories,\n",
        "                                              num_layers=1,\n",
        "                                              dropout=0.0,\n",
        "                                              lr=5e-4,\n",
        "                                              warmup=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8RwhD9DnTTO"
      },
      "source": [
        "The warning of PyTorch Lightning regarding the number of workers can be ignored for now. As the data set is so simple and the `__getitem__` finishes a neglectable time, we don't need subprocesses to provide us the data (in fact, more workers can slow down the training as we have communication overhead among processes/threads). First, let's print the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6Smj5BqnTTO",
        "outputId": "b5058643-ba16-420d-d9bb-ccabb0f81ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val accuracy:  100.00%\n",
            "Test accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Val accuracy:  {(100.0 * reverse_result['val_acc']):4.2f}%\")\n",
        "print(f\"Test accuracy: {(100.0 * reverse_result['test_acc']):4.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfQIwzdGnTTP"
      },
      "source": [
        "As we would have expected, the Transformer can correctly solve the task. However, how does the attention in the Multi-Head Attention block looks like for an arbitrary input? Let's try to visualize it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GPt4TInDnTTP"
      },
      "outputs": [],
      "source": [
        "data_input, labels = next(iter(val_loader))\n",
        "inp_data = F.one_hot(data_input, num_classes=reverse_model.hparams.num_classes).float()\n",
        "inp_data = inp_data.to(device)\n",
        "attention_maps = reverse_model.get_attention_maps(inp_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTUYPNIjnTTP"
      },
      "source": [
        "The object `attention_maps` is a list of length $N$ where $N$ is the number of layers. Each element is a tensor of shape [Batch, Heads, SeqLen, SeqLen], which we can verify below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjppwyRnnTTP",
        "outputId": "d289176c-7516-40a3-8898-cbca2fd119dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 1, 16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "attention_maps[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Lc9YcrnTTP"
      },
      "source": [
        "Next, we will write a plotting function that takes as input the sequences, attention maps, and an index indicating for which batch element we want to visualize the attention map. We will create a plot where over rows, we have different layers, while over columns, we show the different heads. Remember that the softmax has been applied for each row separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "oUSLOharnTTQ"
      },
      "outputs": [],
      "source": [
        "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
        "    if input_data is not None:\n",
        "        input_data = input_data[idx].detach().cpu().numpy()\n",
        "    else:\n",
        "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
        "    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n",
        "\n",
        "    num_heads = attn_maps[0].shape[0]\n",
        "    num_layers = len(attn_maps)\n",
        "    seq_len = input_data.shape[0]\n",
        "    fig_size = 4 if num_heads == 1 else 3\n",
        "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
        "    if num_layers == 1:\n",
        "        ax = [ax]\n",
        "    if num_heads == 1:\n",
        "        ax = [[a] for a in ax]\n",
        "    for row in range(num_layers):\n",
        "        for column in range(num_heads):\n",
        "            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n",
        "            ax[row][column].set_xticks(list(range(seq_len)))\n",
        "            ax[row][column].set_xticklabels(input_data.tolist())\n",
        "            ax[row][column].set_yticks(list(range(seq_len)))\n",
        "            ax[row][column].set_yticklabels(input_data.tolist())\n",
        "            ax[row][column].set_title(f\"Layer {row+1}, Head {column+1}\")\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bBkXMkqnTTQ"
      },
      "source": [
        "Finally, we can plot the attention map of our trained Transformer on the reverse task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "IjBJLLXSnTTQ",
        "outputId": "3ac1fdc5-ef22-4cb0-e13a-ae1d1953a152"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"249.5225pt\" height=\"267.95625pt\" viewBox=\"0 0 249.5225 267.95625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-02-26T18:12:53.188224</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 267.95625 \nL 249.5225 267.95625 \nL 249.5225 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 20.5625 244.078125 \nL 242.3225 244.078125 \nL 242.3225 22.318125 \nL 20.5625 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#pc24cf60451)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"imagec34828524b\" transform=\"scale(1 -1) translate(0 -221.76)\" x=\"20.5625\" y=\"-22.318125\" width=\"221.76\" height=\"221.76\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"md8444a8485\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"27.4925\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 9 -->\n      <g transform=\"translate(24.31125 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-39\" d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"41.3525\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 4 -->\n      <g transform=\"translate(38.17125 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"55.2125\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 7 -->\n      <g transform=\"translate(52.03125 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"69.0725\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(65.89125 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"82.9325\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 3 -->\n      <g transform=\"translate(79.75125 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"96.7925\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(93.61125 258.676562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"110.6525\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1 -->\n      <g transform=\"translate(107.47125 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"124.5125\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 3 -->\n      <g transform=\"translate(121.33125 258.676562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"138.3725\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 6 -->\n      <g transform=\"translate(135.19125 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"152.2325\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 4 -->\n      <g transform=\"translate(149.05125 258.676562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"166.0925\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0 -->\n      <g transform=\"translate(162.91125 258.676562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"179.9525\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2 -->\n      <g transform=\"translate(176.77125 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"193.8125\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 5 -->\n      <g transform=\"translate(190.63125 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"207.6725\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0 -->\n      <g transform=\"translate(204.49125 258.676562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"221.5325\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 8 -->\n      <g transform=\"translate(218.35125 258.676562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#md8444a8485\" x=\"235.3925\" y=\"244.078125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 8 -->\n      <g transform=\"translate(232.21125 258.676562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_17\">\n      <defs>\n       <path id=\"m42c6e11eab\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"237.148125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 9 -->\n      <g transform=\"translate(7.2 240.947344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-39\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"223.288125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 4 -->\n      <g transform=\"translate(7.2 227.087344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"209.428125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 7 -->\n      <g transform=\"translate(7.2 213.227344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-37\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"195.568125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 0 -->\n      <g transform=\"translate(7.2 199.367344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_21\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"181.708125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 3 -->\n      <g transform=\"translate(7.2 185.507344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"167.848125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 0 -->\n      <g transform=\"translate(7.2 171.647344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_23\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"153.988125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 1 -->\n      <g transform=\"translate(7.2 157.787344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_24\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"140.128125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 3 -->\n      <g transform=\"translate(7.2 143.927344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_25\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"126.268125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 6 -->\n      <g transform=\"translate(7.2 130.067344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_26\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"112.408125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 4 -->\n      <g transform=\"translate(7.2 116.207344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_27\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"98.548125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 0 -->\n      <g transform=\"translate(7.2 102.347344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_28\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"84.688125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_28\">\n      <!-- 2 -->\n      <g transform=\"translate(7.2 88.487344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_29\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"70.828125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_29\">\n      <!-- 5 -->\n      <g transform=\"translate(7.2 74.627344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_30\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"56.968125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_30\">\n      <!-- 0 -->\n      <g transform=\"translate(7.2 60.767344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_31\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"43.108125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_31\">\n      <!-- 8 -->\n      <g transform=\"translate(7.2 46.907344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_32\">\n      <g>\n       <use xlink:href=\"#m42c6e11eab\" x=\"20.5625\" y=\"29.248125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_32\">\n      <!-- 8 -->\n      <g transform=\"translate(7.2 33.047344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 20.5625 244.078125 \nL 20.5625 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 242.3225 244.078125 \nL 242.3225 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 20.5625 244.078125 \nL 242.3225 244.078125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 20.5625 22.318125 \nL 242.3225 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_33\">\n    <!-- Layer 1, Head 1 -->\n    <g transform=\"translate(83.760312 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-2c\" d=\"M 750 794 \nL 1409 794 \nL 1409 256 \nL 897 -744 \nL 494 -744 \nL 750 256 \nL 750 794 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-48\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 2753 \nL 3553 2753 \nL 3553 4666 \nL 4184 4666 \nL 4184 0 \nL 3553 0 \nL 3553 2222 \nL 1259 2222 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-4c\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"55.712891\"/>\n     <use xlink:href=\"#DejaVuSans-79\" x=\"116.992188\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"176.171875\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"237.695312\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"278.808594\"/>\n     <use xlink:href=\"#DejaVuSans-31\" x=\"310.595703\"/>\n     <use xlink:href=\"#DejaVuSans-2c\" x=\"374.21875\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"406.005859\"/>\n     <use xlink:href=\"#DejaVuSans-48\" x=\"437.792969\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"512.988281\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"574.511719\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"635.791016\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"699.267578\"/>\n     <use xlink:href=\"#DejaVuSans-31\" x=\"731.054688\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pc24cf60451\">\n   <rect x=\"20.5625\" y=\"22.318125\" width=\"221.76\" height=\"221.76\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "application/pdf": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_attention_maps(data_input, attention_maps, idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubouPEuUnTTQ"
      },
      "source": [
        "The model has learned to attend to the token that is on the flipped index of itself. Hence, it actually does what we intended it to do. We see that it however also pays some attention to values close to the flipped index. This is because the model doesn't need the perfect, hard attention to solve this problem, but is fine with this approximate, noisy attention map. The close-by indices are caused by the similarity of the positional encoding, which we also intended with the positional encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPOBV56wnTTQ"
      },
      "source": [
        "### Set Anomaly Detection\n",
        "\n",
        "Besides sequences, sets are another data structure that is relevant for many applications. In contrast to sequences, elements are unordered in a set. RNNs can only be applied on sets by assuming an order in the data, which however biases the model towards a non-existing order in the data. [Vinyals et al. (2015)](https://arxiv.org/abs/1511.06391) and other papers have shown that the assumed order can have a significant impact on the model's performance, and hence, we should try to not use RNNs on sets. Ideally, our model should be permutation-equivariant/invariant such that the output is the same no matter how we sort the elements in a set.\n",
        "\n",
        "Transformers offer the perfect architecture for this as the Multi-Head Attention is permutation-equivariant, and thus, outputs the same values no matter in what order we enter the inputs (inputs and outputs are permuted equally). The task we are looking at for sets is _Set Anomaly Detection_ which means that we try to find the element(s) in a set that does not fit the others. In the research community, the common application of anomaly detection is performed on a set of images, where $N-1$ images belong to the same category/have the same high-level features while one belongs to another category. Note that category does not necessarily have to relate to a class in a standard classification problem, but could be the combination of multiple features. For instance, on a face dataset, this could be people with glasses, male, beard, etc. An example of distinguishing different animals can be seen below. The first four images show foxes, while the last represents a different animal. We want to recognize that the last image shows a different animal, but it is not relevant which class of animal it is.\n",
        "\n",
        "<center width=\"100%\" style=\"padding:20px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/cifar100_example_anomaly.png?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "In this tutorial, we will use the CIFAR100 dataset. CIFAR100 has 600 images for 100 classes each with a resolution of 32x32, similar to CIFAR10. The larger amount of classes requires the model to attend to specific features in the images instead of coarse features as in CIFAR10, therefore making the task harder. We will show the model a set of 9 images of one class, and 1 image from another class. The task is to find the image that is from a different class than the other images.\n",
        "Using the raw images directly as input to the Transformer is not a good idea, because it is not translation invariant as a CNN, and would need to learn to detect image features from high-dimensional input first of all. Instead, we will use a pre-trained ResNet34 model from the torchvision package to obtain high-level, low-dimensional features of the images. The ResNet model has been pre-trained on the [ImageNet](http://image-net.org/) dataset which contains 1 million images of 1k classes and varying resolutions. However, during training and testing, the images are usually scaled to a resolution of 224x224, and hence we rescale our CIFAR images to this resolution as well. Below, we will load the dataset, and prepare the data for being processed by the ResNet model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzhZrnTVnTTR",
        "outputId": "5525a45a-a2bb-4cf0-d377-2daaaf771866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ../data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:31<00:00, 5380802.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/cifar-100-python.tar.gz to ../data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# ImageNet statistics\n",
        "DATA_MEANS = np.array([0.485, 0.456, 0.406])\n",
        "DATA_STD = np.array([0.229, 0.224, 0.225])\n",
        "# As torch tensors for later preprocessing\n",
        "TORCH_DATA_MEANS = torch.from_numpy(DATA_MEANS).view(1,3,1,1)\n",
        "TORCH_DATA_STD = torch.from_numpy(DATA_STD).view(1,3,1,1)\n",
        "\n",
        "# Resize to 224x224, and normalize to ImageNet statistic\n",
        "transform = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(DATA_MEANS, DATA_STD)\n",
        "                                ])\n",
        "# Loading the training dataset.\n",
        "train_set = CIFAR100(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
        "\n",
        "# Loading the test set\n",
        "test_set = CIFAR100(root=DATASET_PATH, train=False, transform=transform, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mG2NchjnTTR"
      },
      "source": [
        "Next, we want to run the pre-trained ResNet model on the images, and extract the features before the classification layer. These are the most high-level features, and should sufficiently describe the images. CIFAR100 has some similarity to ImageNet, and thus we are not retraining the ResNet model in any form. However, if you would want to get the best performance and have a very large dataset, it would be better to add the ResNet to the computation graph during training and finetune its parameters as well. As we don't have a large enough dataset and want to train our model efficiently, we will extract the features beforehand. Let's load and prepare the model below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJnKI04DnTTR",
        "outputId": "3fdbcb7e-4828-44ad-9795-df325bff761c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to ../saved_models/tutorial6/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 94.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
        "pretrained_model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
        "# Remove classification layer\n",
        "# In some models, it is called \"fc\", others have \"classifier\"\n",
        "# Setting both to an empty sequential represents an identity map of the final features.\n",
        "pretrained_model.fc = nn.Sequential()\n",
        "pretrained_model.classifier = nn.Sequential()\n",
        "# To GPU\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "# Only eval, no gradient required\n",
        "pretrained_model.eval()\n",
        "for p in pretrained_model.parameters():\n",
        "    p.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q32lVi8TnTTR"
      },
      "source": [
        "We will now write a extraction function for the features below. This cell requires access to a GPU, as the model is rather deep and the images relatively large. The GPUs on GoogleColab are sufficient, but running this cell can take 2-3 minutes. Once it is run, the features are exported on disk so they don't have to be recalculated every time you run the notebook. However, this requires >150MB free disk space. So it is recommended to run this only on a local computer if you have enough free disk and a GPU (GoogleColab is fine for this). If you do not have a GPU, you can download the features from the [GoogleDrive folder](https://drive.google.com/drive/folders/1DF7POc6j03pRiWQPWSl5QJX5iY-xK0sV?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "641dbcaa525e4979b2dcc3d597fcd7df",
            "9f9dd41af5fc4cdda596457e7670dac4",
            "f4bb7d9085c14f8c8c33d79e19c3d772",
            "e3e66543aefd4141b6af881ea6378ca0",
            "0f08839e6d4a48c58229bfe28d113bb6",
            "e8bc5cffe002481fb2cbf25eb24ce1b9",
            "117096f59dd048f895726a8b0b7c0787",
            "ffff5291b40c47f9b700549b6dd7230d",
            "f4bd545c86aa470a88024ab7b0469df0",
            "13351807ca6e427e87e5dbc7e811ee20",
            "f4e9caea6188467ab4e0e82b29c1cabe"
          ]
        },
        "id": "YfxoEWtGnTTS",
        "outputId": "58862e4c-cbec-4ddc-a4b9-125c16a1db13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/391 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "641dbcaa525e4979b2dcc3d597fcd7df"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def extract_features(dataset, save_file):\n",
        "    if not os.path.isfile(save_file):\n",
        "        data_loader = data.DataLoader(dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
        "        extracted_features = []\n",
        "        for imgs, _ in tqdm(data_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            feats = pretrained_model(imgs)\n",
        "            extracted_features.append(feats)\n",
        "        extracted_features = torch.cat(extracted_features, dim=0)\n",
        "        extracted_features = extracted_features.detach().cpu()\n",
        "        torch.save(extracted_features, save_file)\n",
        "    else:\n",
        "        extracted_features = torch.load(save_file)\n",
        "    return extracted_features\n",
        "\n",
        "train_feat_file = os.path.join(CHECKPOINT_PATH, \"train_set_features.tar\")\n",
        "train_set_feats = extract_features(train_set, train_feat_file)\n",
        "\n",
        "test_feat_file = os.path.join(CHECKPOINT_PATH, \"test_set_features.tar\")\n",
        "test_feats = extract_features(test_set, test_feat_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16M8b8MinTTS"
      },
      "source": [
        "Let's verify the feature shapes below. The training should have 50k elements, and the test 10k images. The feature dimension is 512 for the ResNet34. If you experiment with other models, you likely see a different feature dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XfmaAXdnTTS"
      },
      "outputs": [],
      "source": [
        "print(\"Train:\", train_set_feats.shape)\n",
        "print(\"Test: \", test_feats.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBFEBiN4nTTS"
      },
      "source": [
        "As usual, we want to create a validation set to detect when we should stop training. In this case, we will split the training set into 90% training, 10% validation. However, the difficulty is here that we need to ensure that the validation set has the same number of images for all 100 labels. Otherwise, we have a class imbalance which is not good for creating the image sets. Hence, we take 10% of the images for each class, and move them into the validation set. The code below does exactly this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ11c3cMnTTS"
      },
      "outputs": [],
      "source": [
        "## Split train into train+val\n",
        "# Get labels from train set\n",
        "labels = train_set.targets\n",
        "\n",
        "# Get indices of images per class\n",
        "labels = torch.LongTensor(labels)\n",
        "num_labels = labels.max()+1\n",
        "sorted_indices = torch.argsort(labels).reshape(num_labels, -1) # [classes, num_imgs per class]\n",
        "\n",
        "# Determine number of validation images per class\n",
        "num_val_exmps = sorted_indices.shape[1] // 10\n",
        "\n",
        "# Get image indices for validation and training\n",
        "val_indices   = sorted_indices[:,:num_val_exmps].reshape(-1)\n",
        "train_indices = sorted_indices[:,num_val_exmps:].reshape(-1)\n",
        "\n",
        "# Group corresponding image features and labels\n",
        "train_feats, train_labels = train_set_feats[train_indices], labels[train_indices]\n",
        "val_feats,   val_labels   = train_set_feats[val_indices],   labels[val_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDM-SCLgnTTT"
      },
      "source": [
        "Now we can prepare a dataset class for the set anomaly task. We define an epoch to be the sequence in which each image has been exactly once as an \"anomaly\". Hence, the length of the dataset is the number of images in it. For the training set, each time we access an item with `__getitem__`, we sample a random, different class than the image at the corresponding index `idx` has. In a second step, we sample $N-1$ images of this sampled class. The set of 10 images is finally returned. The randomness in the `__getitem__` allows us to see a slightly different set during each iteration. However, we can't use the same strategy for the test set as we want the test dataset to be the same every time we iterate over it. Hence, we sample the sets in the `__init__` method, and return those in `__getitem__`. The code below implements exactly this dynamic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlYCSxTsnTTT"
      },
      "outputs": [],
      "source": [
        "class SetAnomalyDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, img_feats, labels, set_size=10, train=True):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            img_feats - Tensor of shape [num_imgs, img_dim]. Represents the high-level features.\n",
        "            labels - Tensor of shape [num_imgs], containing the class labels for the images\n",
        "            set_size - Number of elements in a set. N-1 are sampled from one class, and one from another one.\n",
        "            train - If True, a new set will be sampled every time __getitem__ is called.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.img_feats = img_feats\n",
        "        self.labels = labels\n",
        "        self.set_size = set_size-1 # The set size is here the size of correct images\n",
        "        self.train = train\n",
        "\n",
        "        # Tensors with indices of the images per class\n",
        "        self.num_labels = labels.max()+1\n",
        "        self.img_idx_by_label = torch.argsort(self.labels).reshape(self.num_labels, -1)\n",
        "\n",
        "        if not train:\n",
        "            self.test_sets = self._create_test_sets()\n",
        "\n",
        "\n",
        "    def _create_test_sets(self):\n",
        "        # Pre-generates the sets for each image for the test set\n",
        "        test_sets = []\n",
        "        num_imgs = self.img_feats.shape[0]\n",
        "        np.random.seed(42)\n",
        "        test_sets = [self.sample_img_set(self.labels[idx]) for idx in range(num_imgs)]\n",
        "        test_sets = torch.stack(test_sets, dim=0)\n",
        "        return test_sets\n",
        "\n",
        "\n",
        "    def sample_img_set(self, anomaly_label):\n",
        "        \"\"\"\n",
        "        Samples a new set of images, given the label of the anomaly.\n",
        "        The sampled images come from a different class than anomaly_label\n",
        "        \"\"\"\n",
        "        # Sample class from 0,...,num_classes-1 while skipping anomaly_label as class\n",
        "        set_label = np.random.randint(self.num_labels-1)\n",
        "        if set_label >= anomaly_label:\n",
        "            set_label += 1\n",
        "\n",
        "        # Sample images from the class determined above\n",
        "        img_indices = np.random.choice(self.img_idx_by_label.shape[1], size=self.set_size, replace=False)\n",
        "        img_indices = self.img_idx_by_label[set_label, img_indices]\n",
        "        return img_indices\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.img_feats.shape[0]\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anomaly = self.img_feats[idx]\n",
        "        if self.train: # If train => sample\n",
        "            img_indices = self.sample_img_set(self.labels[idx])\n",
        "        else: # If test => use pre-generated ones\n",
        "            img_indices = self.test_sets[idx]\n",
        "\n",
        "        # Concatenate images. The anomaly is always the last image for simplicity\n",
        "        img_set = torch.cat([self.img_feats[img_indices], anomaly[None]], dim=0)\n",
        "        indices = torch.cat([img_indices, torch.LongTensor([idx])], dim=0)\n",
        "        label = img_set.shape[0]-1\n",
        "\n",
        "        # We return the indices of the images for visualization purpose. \"Label\" is the index of the anomaly\n",
        "        return img_set, indices, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zWaRsiEnTTT"
      },
      "source": [
        "Next, we can setup our datasets and data loaders below. Here, we will use a set size of 10, i.e. 9 images from one category + 1 anomaly. Feel free to change it if you want to experiment with the sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9-fvkAJnTTT"
      },
      "outputs": [],
      "source": [
        "SET_SIZE = 10\n",
        "test_labels = torch.LongTensor(test_set.targets)\n",
        "\n",
        "train_anom_dataset = SetAnomalyDataset(train_feats, train_labels, set_size=SET_SIZE, train=True)\n",
        "val_anom_dataset   = SetAnomalyDataset(val_feats,   val_labels,   set_size=SET_SIZE, train=False)\n",
        "test_anom_dataset  = SetAnomalyDataset(test_feats,  test_labels,  set_size=SET_SIZE, train=False)\n",
        "\n",
        "train_anom_loader = data.DataLoader(train_anom_dataset, batch_size=64, shuffle=True,  drop_last=True,  num_workers=4, pin_memory=True)\n",
        "val_anom_loader   = data.DataLoader(val_anom_dataset,   batch_size=64, shuffle=False, drop_last=False, num_workers=4)\n",
        "test_anom_loader  = data.DataLoader(test_anom_dataset,  batch_size=64, shuffle=False, drop_last=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0c1fqTbnTTT"
      },
      "source": [
        "To understand the dataset a little better, we can plot below a few sets from the test dataset. Each row shows a different input set, where the first 9 are from the same class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezhlda9wnTTT"
      },
      "outputs": [],
      "source": [
        "def visualize_exmp(indices, orig_dataset):\n",
        "    images = [orig_dataset[idx][0] for idx in indices.reshape(-1)]\n",
        "    images = torch.stack(images, dim=0)\n",
        "    images = images * TORCH_DATA_STD + TORCH_DATA_MEANS\n",
        "\n",
        "    img_grid = torchvision.utils.make_grid(images, nrow=SET_SIZE, normalize=True, pad_value=0.5, padding=16)\n",
        "    img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.title(\"Anomaly examples on CIFAR100\")\n",
        "    plt.imshow(img_grid)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "_, indices, _ = next(iter(test_anom_loader))\n",
        "visualize_exmp(indices[:4], test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_ZsXGhcnTTU"
      },
      "source": [
        "We can already see that for some sets the task might be easier than for others. Difficulties can especially arise if the anomaly is in a different, but yet visually similar class (e.g. train vs bus, flour vs worm, etc.).\n",
        "\n",
        "After having prepared the data, we can look closer at the model. Here, we have a classification of the whole set. For the prediction to be permutation-equivariant, we will output one logit for each image. Over these logits, we apply a softmax and train the anomaly image to have the highest score/probability. This is a bit different than a standard classification layer as the softmax is applied over images, not over output classes in the classical sense. However, if we swap two images in their position, we effectively swap their position in the output softmax. Hence, the prediction is equivariant with respect to the input. We implement this idea below in the subclass of the Transformer Lightning module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2sfF5PVnTTU"
      },
      "outputs": [],
      "source": [
        "class AnomalyPredictor(TransformerPredictor):\n",
        "\n",
        "    def _calculate_loss(self, batch, mode=\"train\"):\n",
        "        img_sets, _, labels = batch\n",
        "        preds = self.forward(img_sets, add_positional_encoding=False) # No positional encodings as it is a set, not a sequence!\n",
        "        preds = preds.squeeze(dim=-1) # Shape: [Batch_size, set_size]\n",
        "        loss = F.cross_entropy(preds, labels) # Softmax/CE over set dimension\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "        self.log(f\"{mode}_loss\", loss)\n",
        "        self.log(f\"{mode}_acc\", acc, on_step=False, on_epoch=True)\n",
        "        return loss, acc\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udyepuXLnTTW"
      },
      "source": [
        "Finally, we write our train function below. It has the exact same structure as the reverse task one, hence not much of an explanation is needed here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tGU3czYnTTX"
      },
      "outputs": [],
      "source": [
        "def train_anomaly(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"SetAnomalyTask\")\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=100,\n",
        "                         gradient_clip_val=2)\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"SetAnomalyTask.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = AnomalyPredictor.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        model = AnomalyPredictor(max_iters=trainer.max_epochs*len(train_anom_loader), **kwargs)\n",
        "        trainer.fit(model, train_anom_loader, val_anom_loader)\n",
        "        model = AnomalyPredictor.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Test best model on validation and test set\n",
        "    train_result = trainer.test(model, train_anom_loader, verbose=False)\n",
        "    val_result = trainer.test(model, val_anom_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_anom_loader, verbose=False)\n",
        "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"], \"train_acc\": train_result[0][\"test_acc\"]}\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GIg9L4dnTTX"
      },
      "source": [
        "Let's finally train our model. We will use 4 layers with 4 attention heads each. The hidden dimensionality of the model is 256, and we use a dropout of 0.1 throughout the model for good regularization. Note that we also apply the dropout on the input features, as this makes the model more robust against image noise and generalizes better. Again, we use warmup to slowly start our model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcR6zBcwnTTX"
      },
      "outputs": [],
      "source": [
        "anomaly_model, anomaly_result = train_anomaly(input_dim=train_anom_dataset.img_feats.shape[-1],\n",
        "                                              model_dim=256,\n",
        "                                              num_heads=4,\n",
        "                                              num_classes=1,\n",
        "                                              num_layers=4,\n",
        "                                              dropout=0.1,\n",
        "                                              input_dropout=0.1,\n",
        "                                              lr=5e-4,\n",
        "                                              warmup=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyWPRt-0nTTX"
      },
      "source": [
        "We can print the achieved accuracy below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlE93_JtnTTY"
      },
      "outputs": [],
      "source": [
        "print(f\"Train accuracy: {(100.0*anomaly_result['train_acc']):4.2f}%\")\n",
        "print(f\"Val accuracy:   {(100.0*anomaly_result['val_acc']):4.2f}%\")\n",
        "print(f\"Test accuracy:  {(100.0*anomaly_result['test_acc']):4.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztpv-EJmnTTY"
      },
      "source": [
        "With ~94% validation and test accuracy, the model generalizes quite well. It should be noted that you might see slightly different scores depending on what computer/device you are running this notebook. This is because despite setting the seed before generating the test dataset, it is not the same across platforms and numpy versions. Nevertheless, we can conclude that the model performs quite well and can solve the task for most sets. Before trying to interpret the model, let's verify that our model is permutation-equivariant, and assigns the same predictions for different permutations of the input set. For this, we sample a batch from the test set and run it through the model to obtain the probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJbMuYcsnTTY"
      },
      "outputs": [],
      "source": [
        "inp_data, indices, labels = next(iter(test_anom_loader))\n",
        "inp_data = inp_data.to(device)\n",
        "\n",
        "anomaly_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = anomaly_model.forward(inp_data, add_positional_encoding=False)\n",
        "    preds = F.softmax(preds.squeeze(dim=-1), dim=-1)\n",
        "\n",
        "    # Permut input data\n",
        "    permut = np.random.permutation(inp_data.shape[1])\n",
        "    perm_inp_data = inp_data[:,permut]\n",
        "    perm_preds = anomaly_model.forward(perm_inp_data, add_positional_encoding=False)\n",
        "    perm_preds = F.softmax(perm_preds.squeeze(dim=-1), dim=-1)\n",
        "\n",
        "assert (preds[:,permut] - perm_preds).abs().max() < 1e-5, \"Predictions are not permutation equivariant\"\n",
        "\n",
        "print(\"Preds\\n\", preds[0,permut].cpu().numpy())\n",
        "print(\"Permuted preds\\n\", perm_preds[0].cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKKamqd0nTTY"
      },
      "source": [
        "You can see that the predictions are almost exactly the same, and only differ because of slight numerical differences inside the network operation.\n",
        "\n",
        "To interpret the model a little more, we can plot the attention maps inside the model. This will give us an idea of what information the model is sharing/communicating between images, and what each head might represent. First, we need to extract the attention maps for the test batch above, and determine the discrete predictions for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGk6a7SYnTTY"
      },
      "outputs": [],
      "source": [
        "attention_maps = anomaly_model.get_attention_maps(inp_data, add_positional_encoding=False)\n",
        "predictions = preds.argmax(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOYzeEYlnTTZ"
      },
      "source": [
        "Below we write a plot function which plots the images in the input set, the prediction of the model, and the attention maps of the different heads on layers of the transformer. Feel free to explore the attention maps for different input examples as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0Uzz_0inTTZ"
      },
      "outputs": [],
      "source": [
        "def visualize_prediction(idx):\n",
        "    visualize_exmp(indices[idx:idx+1], test_set)\n",
        "    print(\"Prediction:\", predictions[idx].item())\n",
        "    plot_attention_maps(input_data=None, attn_maps=attention_maps, idx=idx)\n",
        "\n",
        "visualize_prediction(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsRMHE2EnTTZ"
      },
      "source": [
        "Depending on the random seed, you might see a slightly different input set. For the version on the website, we compare 9 tree images with a volcano. We see that multiple heads, for instance, Layer 2 Head 1, Layer 2 Head 3, and Layer 3 Head 1 focus on the last image. Additionally, the heads in Layer 4 all seem to ignore the last image and assign a very low attention probability to it. This shows that the model has indeed recognized that the image doesn't fit the setting, and hence predicted it to be the anomaly. Layer 3 Head 2-4 seems to take a slightly weighted average of all images. That might indicate that the model extracts the \"average\" information of all images, to compare it to the image features itself.\n",
        "\n",
        "Let's try to find where the model actually makes a mistake. We can do this by identifying the sets where the model predicts something else than 9, as in the dataset, we ensured that the anomaly is always at the last position in the set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnIz58QNnTTZ"
      },
      "outputs": [],
      "source": [
        "mistakes = torch.where(predictions != 9)[0].cpu().numpy()\n",
        "print(\"Indices with mistake:\", mistakes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLduXaz0nTTZ"
      },
      "source": [
        "As our model achieves ~94% accuracy, we only have very little number of mistakes in a batch of 64 sets. Still, let's visualize one of them, for example the last one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9LMC4M-nTTa"
      },
      "outputs": [],
      "source": [
        "visualize_prediction(mistakes[-1])\n",
        "print(\"Probabilities:\")\n",
        "for i, p in enumerate(preds[mistakes[-1]].cpu().numpy()):\n",
        "    print(f\"Image {i}: {100.0*p:4.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3DiPVp9nTTa"
      },
      "source": [
        "In this example, the model confuses a palm tree with a building, giving a probability of ~90% to image 2, and 8% to the actual anomaly. However, the difficulty here is that the picture of the building has been taken at a similar angle as the palms. Meanwhile, image 2 shows a rather unusual palm with a different color palette, which is why the model fails here. Nevertheless, in general, the model performs quite well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkQ-cLY9nTTa"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we took a closer look at the Multi-Head Attention layer which uses a scaled dot product between queries and keys to find correlations and similarities between input elements. The Transformer architecture is based on the Multi-Head Attention layer and applies multiple of them in a ResNet-like block. The Transformer is a very important, recent architecture that can be applied to many tasks and datasets. Although it is best known for its success in NLP, there is so much more to it. We have seen its application on sequence-to-sequence tasks and set anomaly detection. Its property of being permutation-equivariant if we do not provide any positional encodings, allows it to generalize to many settings. Hence, it is important to know the architecture, but also its possible issues such as the gradient problem during the first iterations solved by learning rate warm-up. If you are interested in continuing with the study of the Transformer architecture, please have a look at the blog posts listed at the beginning of the tutorial notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhvmm-prnTTa"
      },
      "source": [
        "---\n",
        "\n",
        "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=⭐&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider ⭐-ing our repository.    \n",
        "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=❔&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6b96b702dcb142c3b81000db367cc85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d83ad437a42e412f8b787bdefa7b79ae",
              "IPY_MODEL_0436637c8d95419a8f3f07875a8cd975",
              "IPY_MODEL_9a0564266c534d159f31696e98fde9b4"
            ],
            "layout": "IPY_MODEL_b14c7e8218be4c72b4ebab4fe8da8489"
          }
        },
        "d83ad437a42e412f8b787bdefa7b79ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12368f3f029a474d8e8236274fce948f",
            "placeholder": "​",
            "style": "IPY_MODEL_4bea47bf280949c6af21d04929fe5445",
            "value": "Testing DataLoader 0: 100%"
          }
        },
        "0436637c8d95419a8f3f07875a8cd975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fea821813b2e492eb32f60b3b5bb6596",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d6cba062aa247bc91e2d5f43a5c5fd0",
            "value": 8
          }
        },
        "9a0564266c534d159f31696e98fde9b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7466b3b5a01436c805347f39fe70a99",
            "placeholder": "​",
            "style": "IPY_MODEL_b4123d11e31c4fddbb55cd3339c91a76",
            "value": " 8/8 [00:00&lt;00:00, 24.71it/s]"
          }
        },
        "b14c7e8218be4c72b4ebab4fe8da8489": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "12368f3f029a474d8e8236274fce948f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bea47bf280949c6af21d04929fe5445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fea821813b2e492eb32f60b3b5bb6596": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d6cba062aa247bc91e2d5f43a5c5fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7466b3b5a01436c805347f39fe70a99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4123d11e31c4fddbb55cd3339c91a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1a24b0a638143d8aec3a439394994c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f69435bbbb046be929f6047a35d4c1d",
              "IPY_MODEL_858431d99a35482dad1c7d9f309739fd",
              "IPY_MODEL_4c9bab5bf0244e4ba4fdc5b4e713fb1f"
            ],
            "layout": "IPY_MODEL_c9369a156d1047c6a3823577f00bfdbc"
          }
        },
        "8f69435bbbb046be929f6047a35d4c1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5919278278024223851fd7a0c9a79377",
            "placeholder": "​",
            "style": "IPY_MODEL_22ca997ab9584f6db2ab39c2a5e49674",
            "value": "Testing DataLoader 0: 100%"
          }
        },
        "858431d99a35482dad1c7d9f309739fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc9a90857516499b9c6ef7a9ea84a245",
            "max": 79,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3771c3751ab465c8e4374af0865019c",
            "value": 79
          }
        },
        "4c9bab5bf0244e4ba4fdc5b4e713fb1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb04ccdf4a2a4b6b8bb279ffec0123be",
            "placeholder": "​",
            "style": "IPY_MODEL_14e5acbc13744710928b579c14d641ed",
            "value": " 79/79 [00:02&lt;00:00, 33.69it/s]"
          }
        },
        "c9369a156d1047c6a3823577f00bfdbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "5919278278024223851fd7a0c9a79377": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22ca997ab9584f6db2ab39c2a5e49674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc9a90857516499b9c6ef7a9ea84a245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3771c3751ab465c8e4374af0865019c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb04ccdf4a2a4b6b8bb279ffec0123be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14e5acbc13744710928b579c14d641ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "641dbcaa525e4979b2dcc3d597fcd7df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f9dd41af5fc4cdda596457e7670dac4",
              "IPY_MODEL_f4bb7d9085c14f8c8c33d79e19c3d772",
              "IPY_MODEL_e3e66543aefd4141b6af881ea6378ca0"
            ],
            "layout": "IPY_MODEL_0f08839e6d4a48c58229bfe28d113bb6"
          }
        },
        "9f9dd41af5fc4cdda596457e7670dac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8bc5cffe002481fb2cbf25eb24ce1b9",
            "placeholder": "​",
            "style": "IPY_MODEL_117096f59dd048f895726a8b0b7c0787",
            "value": " 30%"
          }
        },
        "f4bb7d9085c14f8c8c33d79e19c3d772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffff5291b40c47f9b700549b6dd7230d",
            "max": 391,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4bd545c86aa470a88024ab7b0469df0",
            "value": 118
          }
        },
        "e3e66543aefd4141b6af881ea6378ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13351807ca6e427e87e5dbc7e811ee20",
            "placeholder": "​",
            "style": "IPY_MODEL_f4e9caea6188467ab4e0e82b29c1cabe",
            "value": " 118/391 [41:44&lt;1:46:30, 23.41s/it]"
          }
        },
        "0f08839e6d4a48c58229bfe28d113bb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8bc5cffe002481fb2cbf25eb24ce1b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "117096f59dd048f895726a8b0b7c0787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffff5291b40c47f9b700549b6dd7230d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4bd545c86aa470a88024ab7b0469df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13351807ca6e427e87e5dbc7e811ee20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4e9caea6188467ab4e0e82b29c1cabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}